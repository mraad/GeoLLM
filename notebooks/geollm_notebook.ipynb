{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f89968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import io\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import Callable\n",
    "from urllib.parse import urlparse\n",
    "import pandas\n",
    "import requests\n",
    "import tiktoken\n",
    "import geopandas as gp\n",
    "from tiktoken import Encoding\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from langchain import BasePromptTemplate, GoogleSearchAPIWrapper, LLMChain\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "from langchain.chat_models import ChatOpenAI, AzureChatOpenAI\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.functions import col, unix_timestamp, from_unixtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7109cb2",
   "metadata": {},
   "source": [
    "## AI Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7103a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Type\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "class AIMethodWrapper:\n",
    "    \"\"\"\n",
    "    This class wraps the AI utility functions to allow them to be used directly\n",
    "    on DataFrame instances. An instance of this class is created each time the\n",
    "    utility functions are accessed, with the DataFrame and SparkAI instance\n",
    "    passed to it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, spark_ai, df_instance: DataFrame):\n",
    "        \"\"\"\n",
    "        Initialize the AIMethodWrapper with the given SparkAI and DataFrame instance.\n",
    "\n",
    "        Args:\n",
    "            spark_ai: The SparkAI instance containing the AI utility methods.\n",
    "            df_instance: The DataFrame instance on which the utility methods will be used.\n",
    "        \"\"\"\n",
    "        self.spark_ai = spark_ai\n",
    "        self.df_instance = df_instance\n",
    "\n",
    "    def transform(self, desc: str, cache: bool = True) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Transform the DataFrame using the given description.\n",
    "\n",
    "        Args:\n",
    "            desc: A string description specifying the transformation.\n",
    "            cache: Indicates whether to utilize a cache for this method.\n",
    "                If `True`, fetches cached data, if available.\n",
    "                If `False`, retrieves fresh data and updates cache.\n",
    "\n",
    "        Returns:\n",
    "            The transformed DataFrame.\n",
    "        \"\"\"\n",
    "        return self.spark_ai.transform_df(self.df_instance, desc, cache)\n",
    "\n",
    "    def explain(self, cache: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Explain the DataFrame.\n",
    "\n",
    "        Args:\n",
    "            cache: Indicates whether to utilize a cache for this method.\n",
    "                If `True`, fetches cached data, if available.\n",
    "                If `False`, retrieves fresh data and updates cache.\n",
    "\n",
    "        Returns:\n",
    "            A string explanation of the DataFrame.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.spark_ai.explain_df(self.df_instance, cache)\n",
    "\n",
    "    def plot(self, desc: Optional[str] = None, cache: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Plot the DataFrame.\n",
    "\n",
    "        Args:\n",
    "            desc: A string description specifying the plot.\n",
    "            cache: Indicates whether to utilize a cache for this method.\n",
    "                If `True`, fetches cached data, if available.\n",
    "                If `False`, retrieves fresh data and updates cache.\n",
    "        \"\"\"\n",
    "        return self.spark_ai.plot_df(self.df_instance, desc, cache)\n",
    "\n",
    "    def verify(self, desc: str, cache: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Verify the DataFrame using the given description.\n",
    "\n",
    "        Args:\n",
    "            desc: A string description specifying what to verify in the DataFrame.\n",
    "            cache: Indicates whether to utilize a cache for this method.\n",
    "                If `True`, fetches cached data, if available.\n",
    "                If `False`, retrieves fresh data and updates cache.\n",
    "        \"\"\"\n",
    "        return self.spark_ai.verify_df(self.df_instance, desc, cache)\n",
    "\n",
    "\n",
    "class AIUtils:\n",
    "    \"\"\"\n",
    "    This class is a descriptor that is used to add AI utility methods to DataFrame instances.\n",
    "    When the utility methods are accessed, it returns a new AIMethodWrapper instance with the\n",
    "    DataFrame and SparkAI instance passed to it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, spark_ai):\n",
    "        \"\"\"\n",
    "        Initialize the AIUtils descriptor with the given SparkAI.\n",
    "\n",
    "        Args:\n",
    "            spark_ai: The SparkAI instance containing the AI utility methods.\n",
    "        \"\"\"\n",
    "        self.spark_ai = spark_ai\n",
    "\n",
    "    def __get__(self, instance: DataFrame, owner: Type[DataFrame]) -> AIMethodWrapper:\n",
    "        \"\"\"\n",
    "        This method is called when the AI utility methods are accessed on a DataFrame instance.\n",
    "        It returns a new AIMethodWrapper instance with the DataFrame instance and SparkAI passed to it.\n",
    "\n",
    "        Args:\n",
    "            instance: The DataFrame instance on which the utility methods are being accessed.\n",
    "            owner: The class (DataFrame) to which this descriptor is added.\n",
    "\n",
    "        Returns:\n",
    "            A new AIMethodWrapper instance.\n",
    "        \"\"\"\n",
    "        return AIMethodWrapper(self.spark_ai, instance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d494aed8",
   "metadata": {},
   "source": [
    "## File Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31af0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "from langchain.cache import SQLiteCache\n",
    "from langchain.schema import Generation\n",
    "\n",
    "\n",
    "class FileCache(ABC):\n",
    "    \"\"\"Base interface for a file-based cache.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def lookup(self, key: str) -> Optional[str]:\n",
    "        \"\"\"Perform a lookup based on the key.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, key: str, val: str) -> None:\n",
    "        \"\"\"Update cache based on the key.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def clear(self, **kwargs: Any) -> None:\n",
    "        \"\"\"Clear cache. Can take additional keyword arguments.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def commit_staging_cache(self, staging_cache: Dict[str, str]) -> None:\n",
    "        \"\"\"Commit all items from the staging_cache to the cache.\"\"\"\n",
    "\n",
    "\n",
    "class SQLiteCacheWrapper(FileCache):\n",
    "    \"\"\"Wrapper class for SQLiteCache that ignores llm_string during lookups and updates.\"\"\"\n",
    "\n",
    "    def __init__(self, cache_file_location: str):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the SQLiteCacheWrapper class.\n",
    "\n",
    "        Args:\n",
    "            cache_file_location (str): The SQLite file location\n",
    "        \"\"\"\n",
    "        self._sqlite_cache = SQLiteCache(database_path=cache_file_location)\n",
    "\n",
    "    def lookup(self, key: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Performs a lookup in the SQLiteCache using the given key.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key string for the lookup.\n",
    "\n",
    "        Returns:\n",
    "            Optional[RETURN_VAL_TYPE]: The cached value corresponding to the key, if available. Otherwise, None.\n",
    "        \"\"\"\n",
    "        lookup_result = self._sqlite_cache.lookup(prompt=key, llm_string=\"\")\n",
    "        if lookup_result is not None and len(lookup_result) > 0:\n",
    "            return lookup_result[0].text\n",
    "        return None\n",
    "\n",
    "    def update(self, key: str, val: str) -> None:\n",
    "        \"\"\"\n",
    "        Updates the SQLiteCache with the given key and return value.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key string.\n",
    "            val (RETURN_VAL_TYPE): The return value to be cached.\n",
    "        \"\"\"\n",
    "        stored_value = [Generation(text=val)]\n",
    "        self._sqlite_cache.update(key, \"\", stored_value)\n",
    "\n",
    "    def clear(self, **kwargs: Any) -> None:\n",
    "        \"\"\"\n",
    "        Clears the SQLiteCache.\n",
    "\n",
    "        Args:\n",
    "            **kwargs: Additional keyword arguments for the clear method of SQLiteCache.\n",
    "        \"\"\"\n",
    "        self._sqlite_cache.clear(**kwargs)\n",
    "\n",
    "    def commit_staging_cache(self, staging_cache: Dict[str, str]) -> None:\n",
    "        \"\"\"\n",
    "        Commits all items from the staging_cache to the SQLiteCache.\n",
    "\n",
    "        Args:\n",
    "            staging_cache (Dict[str, str]): The staging cache to be committed.\n",
    "        \"\"\"\n",
    "        for key, value in staging_cache.items():\n",
    "            self.update(key, value)\n",
    "\n",
    "\n",
    "class JsonCache(FileCache):\n",
    "    \"\"\"A simple caching system using a JSON file for storage, subclass of FileCache.\"\"\"\n",
    "\n",
    "    def __init__(self, filepath: str):\n",
    "        \"\"\"Initialize a new JsonCache instance.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): The path to the JSON file to use for the cache.\n",
    "        \"\"\"\n",
    "        self.filepath = filepath\n",
    "        # If cache file exists, load it into memory.\n",
    "        self.cache = {}\n",
    "        if os.path.exists(self.filepath):\n",
    "            with open(self.filepath, \"r\") as f:\n",
    "                for line in f:\n",
    "                    if line.strip():  # Avoid empty lines\n",
    "                        line_cache = json.loads(line)\n",
    "                        self.cache[line_cache[\"key\"]] = line_cache[\"value\"]\n",
    "        # Create an empty staging cache for storing changes before they are\n",
    "        # committed.\n",
    "        self.staging_cache: Dict = {}\n",
    "\n",
    "    def update(self, key: str, value: str) -> None:\n",
    "        \"\"\"Store a value in the cache for a given key.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key string.\n",
    "            value (RETURN_VAL_TYPE): The value to store in the cache.\n",
    "        \"\"\"\n",
    "        # Store the value in the staging cache.\n",
    "        self.staging_cache[key] = value\n",
    "\n",
    "    def lookup(self, key: str) -> Optional[str]:\n",
    "        \"\"\"Retrieve a value from the cache for a given key.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key string.\n",
    "\n",
    "        Returns:\n",
    "            Optional[RETURN_VAL_TYPE]: The cached value for the given key, or None if no such value exists.\n",
    "        \"\"\"\n",
    "        return self.cache.get(key)\n",
    "\n",
    "    def commit_staging_cache(self, staging_cache: Dict[str, str]) -> None:\n",
    "        \"\"\"Commit all changes in the staging cache to the cache file.\n",
    "\n",
    "        This method writes all changes in the staging cache to the end of the cache file and then clears\n",
    "        the staging cache.\n",
    "\n",
    "        Args:\n",
    "            staging_cache (Dict[str, str]): The staging cache to be committed.\n",
    "        \"\"\"\n",
    "        # Append the staging cache to the existing cache\n",
    "        self.cache.update(staging_cache)\n",
    "        with open(self.filepath, \"a\") as f:\n",
    "            for key, value in staging_cache.items():\n",
    "                json.dump({\"key\": key, \"value\": value}, f)\n",
    "                f.write(\"\\n\")\n",
    "        # Clear the staging cache\n",
    "        self.staging_cache = {}\n",
    "\n",
    "    def clear(self, **kwargs: Any) -> None:\n",
    "        \"\"\"Clear the cache.\n",
    "\n",
    "        This method removes all entries from the cache and deletes the cache file.\n",
    "        \"\"\"\n",
    "        self.cache = {}\n",
    "        self.staging_cache = {}\n",
    "        os.remove(self.filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6867a5a4",
   "metadata": {},
   "source": [
    "## Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc812e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "\n",
    "JsonCache, SQLiteCacheWrapper, FileCache\n",
    "\n",
    "\n",
    "class Cache:\n",
    "    \"\"\"\n",
    "    This class provides an interface for a simple in-memory and persistent cache system. It keeps an in-memory staging\n",
    "    cache, which gets updated through the `update` method and can be persisted through the `commit` method. Cache\n",
    "    lookup is first performed on the in-memory staging cache, and if not found, it is performed on the persistent\n",
    "    cache.\n",
    "\n",
    "    Attributes:\n",
    "        _staging_updates: A dictionary to keep track of the in-memory staging updates.\n",
    "        _file_cache: An instance of either JsonCache or SQLiteCacheWrapper that acts as the persistent cache.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, cache_file_location: str = \".pyspark_ai.json\", file_format: str = \"json\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the Cache class.\n",
    "\n",
    "        Args:\n",
    "            cache_file_location (str, optional): The path to the cache file for the JsonCache or SQLiteCacheWrapper.\n",
    "                Defaults to \".pyspark_ai.json\".\n",
    "            file_format (str, optional): The format of the file to use for the cache. Defaults to \"json\".\n",
    "        \"\"\"\n",
    "        self._staging_updates: Dict[str, str] = {}\n",
    "        if file_format == \"json\":\n",
    "            self._file_cache: FileCache = JsonCache(cache_file_location)\n",
    "        else:\n",
    "            self._file_cache = SQLiteCacheWrapper(cache_file_location)\n",
    "\n",
    "    def lookup(self, key: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Performs a lookup in the cache using the given key.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key string for the lookup.\n",
    "\n",
    "        Returns:\n",
    "            Optional[str]: The cached text corresponding to the key, if available. Otherwise, None.\n",
    "        \"\"\"\n",
    "        # First look in the staging cache\n",
    "        staging_result = self._staging_updates.get(key)\n",
    "        if staging_result is not None:\n",
    "            return staging_result\n",
    "        # If not found in staging cache, look in the persistent cache\n",
    "        return self._file_cache.lookup(key)\n",
    "\n",
    "    def update(self, key: str, val: str) -> None:\n",
    "        \"\"\"\n",
    "        Updates the staging cache with the given key and value.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key string.\n",
    "            val (str): The value to be cached.\n",
    "        \"\"\"\n",
    "        self._staging_updates[key] = val\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"\n",
    "        Clears both the in-memory staging cache and the persistent cache.\n",
    "        \"\"\"\n",
    "        self._file_cache.clear()\n",
    "        self._staging_updates = {}\n",
    "\n",
    "    def commit(self) -> None:\n",
    "        \"\"\"\n",
    "        Commits all the staged updates to the persistent cache.\n",
    "        \"\"\"\n",
    "        self._file_cache.commit_staging_cache(self._staging_updates)\n",
    "        self._staging_updates = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8102c2",
   "metadata": {},
   "source": [
    "## Code Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315bc7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from pygments import highlight\n",
    "from pygments.formatters import TerminalFormatter\n",
    "from pygments.lexers import PythonLexer, SqlLexer\n",
    "\n",
    "GREEN = \"\\033[92m\"  # terminal code for green\n",
    "RESET = \"\\033[0m\"  # reset terminal color\n",
    "\n",
    "\n",
    "# Custom Formatter\n",
    "class CustomFormatter(logging.Formatter):\n",
    "    def format(self, record):\n",
    "        return GREEN + \"INFO: \" + RESET + super().format(record)\n",
    "\n",
    "\n",
    "class CodeLogger:\n",
    "    def __init__(self, name):\n",
    "        self.logger = logging.getLogger(name)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        if not self.logger.handlers:\n",
    "            handler = logging.StreamHandler(sys.stdout)\n",
    "            handler.setFormatter(\n",
    "                CustomFormatter(\"%(message)s\")\n",
    "            )  # output only the message\n",
    "            self.logger.addHandler(handler)\n",
    "\n",
    "    @staticmethod\n",
    "    def colorize_code(code, language):\n",
    "        if not language or language.lower() == \"python\":\n",
    "            lexer = PythonLexer()\n",
    "        elif language.lower() == \"sql\":\n",
    "            lexer = SqlLexer()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported language: {language}\")\n",
    "        return highlight(code, lexer, TerminalFormatter())\n",
    "\n",
    "    def log(self, message):\n",
    "        # Define pattern to match code blocks with optional language specifiers\n",
    "        pattern = r\"```(python|sql)?(.*?)```\"\n",
    "        # Split message into parts. Every 3rd part will be a code block.\n",
    "        parts = re.split(pattern, message, flags=re.DOTALL)\n",
    "\n",
    "        colored_message = \"\"\n",
    "        for i in range(0, len(parts), 3):\n",
    "            # Add regular text to the message\n",
    "            colored_message += parts[i]\n",
    "            # If there is a code block, colorize it and add it to the message\n",
    "            if i + 2 < len(parts):\n",
    "                colored_message += (\n",
    "                    \"\\n```\\n\" + self.colorize_code(parts[i + 2], parts[i + 1]) + \"```\"\n",
    "                )\n",
    "        # Log the message with colored code blocks\n",
    "        self.logger.info(colored_message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5bc591",
   "metadata": {},
   "source": [
    "## Temp View Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04227b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "\n",
    "prefix = \"spark_ai_temp_view\"\n",
    "pattern = f\"{prefix}_[0-9a-zA-Z]{{6}}\"\n",
    "\n",
    "\n",
    "def random_view_name() -> str:\n",
    "    \"\"\"\n",
    "    Generate a random temp view name.\n",
    "    \"\"\"\n",
    "    return f\"{prefix}_{uuid.uuid4().hex[:6]}\"\n",
    "\n",
    "\n",
    "def canonize_string(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace all occurrences of 'spark_ai_temp_view' followed by 6 alphanumeric characters with 'spark_ai_temp_view'\n",
    "     in a given string.\n",
    "\n",
    "    Args:\n",
    "        s (str): The string in which to replace substrings.\n",
    "\n",
    "    Returns:\n",
    "        str: The modified string with all matching substrings replaced.\n",
    "    \"\"\"\n",
    "    return re.sub(pattern, prefix, s)\n",
    "\n",
    "\n",
    "def replace_view_name(s: str, random_view: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace all the 'spark_ai_temp_view' followed by 6 alphanumeric characters in a given string with a random view\n",
    "     name.\n",
    "    \"\"\"\n",
    "    return re.sub(pattern, random_view, s)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c31e65",
   "metadata": {},
   "source": [
    "## LLM Chain with Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce858f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain.callbacks.manager import Callbacks\n",
    "\n",
    "Cache\n",
    "canonize_string\n",
    "\n",
    "SKIP_CACHE_TAGS = [\"SKIP_CACHE\"]\n",
    "\n",
    "\n",
    "class LLMChainWithCache(LLMChain):\n",
    "    cache: Cache\n",
    "\n",
    "    @staticmethod\n",
    "    def _sort_and_stringify(*args: Any) -> str:\n",
    "        # Convert all arguments to strings, then sort them\n",
    "        sorted_args = sorted(str(arg) for arg in args)\n",
    "        # Join all the sorted, stringified arguments with a space\n",
    "        result = \" \".join(sorted_args)\n",
    "        return result\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        *args: Any,\n",
    "        callbacks: Callbacks = None,\n",
    "        tags: Optional[List[str]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        assert not args, \"The chain expected no arguments\"\n",
    "        prompt_str = canonize_string(self.prompt.format_prompt(**kwargs).to_string())\n",
    "        use_cache = tags != SKIP_CACHE_TAGS\n",
    "        cached_result = self.cache.lookup(prompt_str) if use_cache else None\n",
    "        if cached_result is not None:\n",
    "            return cached_result\n",
    "        result = super().run(*args, callbacks=callbacks, tags=tags, **kwargs)\n",
    "        if use_cache:\n",
    "            self.cache.update(prompt_str, result)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1e0624",
   "metadata": {},
   "source": [
    "## Search Tool with Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f271747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "Cache\n",
    "\n",
    "\n",
    "class SearchToolWithCache:\n",
    "    def __init__(self, web_search_tool: Callable[[str], str], cache: Cache):\n",
    "        self.web_search_tool = web_search_tool\n",
    "        self.cache = cache\n",
    "\n",
    "    def search(self, query: str) -> str:\n",
    "        # Try to get the result from the cache\n",
    "        key = f\"web_search:{query}\"\n",
    "        cached_result = self.cache.lookup(key)\n",
    "        if cached_result is not None:\n",
    "            return cached_result\n",
    "\n",
    "        # If the result was not in the cache, use the web_search_tool\n",
    "        result = self.web_search_tool(query)\n",
    "\n",
    "        # Update the cache with the new result\n",
    "        self.cache.update(key, result)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4428d332",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3234d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flake8: noqa\n",
    "from langchain import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "SEARCH_TEMPLATE = \"\"\"Given a Query and a list of Google Search Results, return the link\n",
    "from a reputable website which contains the data set to answer the question. {columns}\n",
    "Query:{query}\n",
    "Google Search Results:\n",
    "```\n",
    "{search_results}\n",
    "```\n",
    "The answer MUST contain the url link only\n",
    "\"\"\"\n",
    "\n",
    "SEARCH_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"search_results\", \"columns\"], template=SEARCH_TEMPLATE\n",
    ")\n",
    "\n",
    "SQL_TEMPLATE = \"\"\"Given the following question:\n",
    "```\n",
    "{query}\n",
    "```\n",
    "I got the following answer from a web page:\n",
    "```\n",
    "{web_content}\n",
    "```\n",
    "Now help me write a SQL query to store the answer into a temp view.\n",
    "Give each column a clearly descriptive name (no abbreviations).\n",
    "If a column can be either String or Numeric, ingest it as Numeric.\n",
    "Here is an example of how to store data into the temp view {view_name}:\n",
    "```\n",
    "CREATE OR REPLACE TEMP VIEW {view_name} AS SELECT * FROM VALUES('Citizen Kane', 1941), ('Schindler\\'s List', 1993) AS v1(title, year)\n",
    "```\n",
    "{columns}\n",
    "The answer MUST contain query only and the temp view MUST be {view_name}.\n",
    "\"\"\"\n",
    "\n",
    "SQL_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"web_content\", \"view_name\", \"columns\"],\n",
    "    template=SQL_TEMPLATE,\n",
    ")\n",
    "\n",
    "TRANSFORM_TEMPLATE = \"\"\"\n",
    "Given a Spark temp view `{view_name}` with the following columns:\n",
    "```\n",
    "{columns}\n",
    "```\n",
    "Write a Spark SQL query to retrieve: {desc}\n",
    "The answer MUST contain query only. Ensure your answer is correct.\n",
    "\"\"\"\n",
    "\n",
    "TRANSFORM_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"view_name\", \"columns\", \"desc\"], template=TRANSFORM_TEMPLATE\n",
    ")\n",
    "\n",
    "EXPLAIN_PREFIX = \"\"\"You are an Apache Spark SQL expert, who can summary what a dataframe retrieves. Given an analyzed\n",
    "query plan of a dataframe, you will\n",
    "1. convert the dataframe to SQL query. Note that an explain output contains plan\n",
    "nodes separated by `\\\\n`. Each plan node has its own expressions and expression ids.\n",
    "2. summary what the sql query retrieves.\n",
    "\"\"\"\n",
    "\n",
    "EXPLAIN_SUFFIX = \"analyzed_plan: {input}\\nexplain:\"\n",
    "\n",
    "_plan1 = \"\"\"\n",
    "GlobalLimit 100\n",
    "    +- LocalLimit 100\n",
    "       +- Sort [d_year ASC NULLS FIRST, sum_agg DESC NULLS LAST, brand_id ASC NULLS FIRST], true\n",
    "          +- Aggregate [d_year, i_brand, i_brand_id], [d_year, i_brand_id AS brand_id, i_brand AS brand, sum(ss_ext_sales_price) AS sum_agg]\n",
    "             +- Filter (((d_date_sk = ss_sold_date_sk) AND (ss_item_sk = i_item_sk)) AND ((i_manufact_id = 128) AND (d_moy = 11)))\n",
    "                +- Join Inner\n",
    "                   :- Join Inner\n",
    "                   :  :- SubqueryAlias dt\n",
    "                   :  :  +- SubqueryAlias spark_catalog.tpcds_sf1_delta.date_dim\n",
    "                   :  :     +- Relation spark_catalog.tpcds_sf1_delta.date_dim[d_date_sk,d_date_id,d_date,d_month_seq,d_week_seq,d_quarter_seq,d_year,d_dow,d_moy,d_dom,d_qoy,d_fy_year,d_fy_quarter_seq,d_fy_week_seq,d_day_name,d_quarter_name,d_holiday,d_weekend,d_following_holiday,d_first_dom,d_last_dom,d_same_day_ly,d_same_day_lq,d_current_day,... 4 more fields] parquet\n",
    "                   :  +- SubqueryAlias spark_catalog.tpcds_sf1_delta.store_sales\n",
    "                   :     +- Relation spark_catalog.tpcds_sf1_delta.store_sales[ss_sold_date_sk,ss_sold_time_sk,ss_item_sk,ss_customer_sk,ss_cdemo_sk,ss_hdemo_sk,ss_addr_sk,ss_store_sk,ss_promo_sk,ss_ticket_numberL,ss_quantity,ss_wholesale_cost,ss_list_price,ss_sales_price,ss_ext_discount_amt,ss_ext_sales_price,ss_ext_wholesale_cost,ss_ext_list_price,ss_ext_tax,ss_coupon_amt,ss_net_paid,ss_net_paid_inc_tax,ss_net_profit] parquet\n",
    "                   +- SubqueryAlias spark_catalog.tpcds_sf1_delta.item\n",
    "                      +- Relation spark_catalog.tpcds_sf1_delta.item[i_item_sk,i_item_id,i_rec_start_date,i_rec_end_date,i_item_desc,i_current_price,i_wholesale_cost,i_brand_id,i_brand,i_class_id,i_class,i_category_id,i_category,i_manufact_id,i_manufact,i_size,i_formulation,i_color,i_units,i_container,i_manager_id,i_product_name] parquet\n",
    "\"\"\"\n",
    "\n",
    "_explain1 = \"\"\"\n",
    "The analyzed plan can be translated into the following SQL query:\n",
    "```sql\n",
    "SELECT\n",
    "  dt.d_year,\n",
    "  item.i_brand_id brand_id,\n",
    "  item.i_brand brand,\n",
    "  SUM(ss_ext_sales_price) sum_agg\n",
    "FROM date_dim dt, store_sales, item\n",
    "WHERE dt.d_date_sk = store_sales.ss_sold_date_sk\n",
    "  AND store_sales.ss_item_sk = item.i_item_sk\n",
    "  AND item.i_manufact_id = 128\n",
    "  AND dt.d_moy = 11\n",
    "GROUP BY dt.d_year, item.i_brand, item.i_brand_id\n",
    "ORDER BY dt.d_year, sum_agg DESC, brand_id\n",
    "LIMIT 100\n",
    "```\n",
    "In summary, this dataframe is retrieving the top 100 brands (specifically of items manufactured by manufacturer with id 128) with the highest total sales price for each year in the month of November. It presents the results sorted by year, total sales (in descending order), and brand id.\n",
    "\"\"\"\n",
    "\n",
    "_explain_examples = [{\"analyzed_plan\": _plan1, \"explain\": _explain1}]\n",
    "\n",
    "_example_formatter = \"\"\"\n",
    "analyzed_plan: {analyzed_plan}\n",
    "explain: {explain}\n",
    "\"\"\"\n",
    "\n",
    "_example_prompt = PromptTemplate(\n",
    "    input_variables=[\"analyzed_plan\", \"explain\"], template=_example_formatter\n",
    ")\n",
    "\n",
    "EXPLAIN_DF_PROMPT = FewShotPromptTemplate(\n",
    "    examples=_explain_examples,\n",
    "    example_prompt=_example_prompt,\n",
    "    prefix=EXPLAIN_PREFIX,\n",
    "    suffix=EXPLAIN_SUFFIX,\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\\n\",\n",
    ")\n",
    "\n",
    "PLOT_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an Apache Spark SQL expert programmer.\n",
    "It is forbidden to include old deprecated APIs in your code.\n",
    "For example, you will not use the pandas method \"append\" because it is deprecated.\n",
    "\n",
    "Given a pyspark DataFrame `df`, with the output columns:\n",
    "{columns}\n",
    "\n",
    "And an explanation of `df`: {explain}\n",
    "\n",
    "Write Python code to visualize the result of `df` using plotly. Make sure to use the exact column names of `df`.\n",
    "Your code may NOT contain \"append\" anywhere. Instead of append, use pd.concat.\n",
    "There is no need to install any package with pip. Do include any necessary import statements.\n",
    "Display the plot directly, instead of saving into an HTML.\n",
    "Do not use scatter plot to display any kind of percentage data.\n",
    "You must import and start your Spark session if you use a Spark DataFrame.\n",
    "Remember to ensure that your code does NOT include \"append\" anywhere, under any circumstance (use pd.concat instead).\n",
    "There is no need to read from a csv file. You can use directly the DataFrame `df` as input.\n",
    "Ensure that your code is correct.\n",
    "{instruction}\n",
    "\"\"\"\n",
    "\n",
    "PLOT_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"columns\", \"explain\", \"instruction\"], template=PLOT_PROMPT_TEMPLATE\n",
    ")\n",
    "\n",
    "GEOBINS_PLOT_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an Apache Spark SQL expert programmer with knowledge in spatial operations.\n",
    "It is forbidden to include old deprecated APIs in your code.\n",
    "\n",
    "Given a pyspark DataFrame `df`,with the output columns:\n",
    "{columns}\n",
    "\n",
    "And an explanation of `df`: {explain}\n",
    "\n",
    "Write Python code to perform spatial binning.\n",
    "Run help on the following custom library geofunctions S to understand how to use it.\n",
    "There is no need to install any package with pip. Do include any necessary import statements.\n",
    "{instruction}\n",
    "\n",
    "Extract the following variables from the instructions and use them in your sample code below:\n",
    "1. <lon column> (the name of the longitude column)\n",
    "2. <lat column> (the name of the latitude column)\n",
    "3. <cell size> (the size of the spatial bin cell)\n",
    "4. <max count> (the maximum count of the spatial bin cell)\n",
    "\n",
    "\n",
    "sample code:\n",
    "\n",
    "import geofunctions as S\n",
    "import pyspark.sql.functions as F\n",
    "import geopandas as gp\n",
    "\n",
    "#Perform Spatial Binning\n",
    "df = (\n",
    "    df.select(S.st_lontoq(\"<lon column>\", cell), S.st_lattor(\"<lat column>\", <cell size>))\n",
    "    .groupBy(\"q\", \"r\")\n",
    "    .count()\n",
    "    .select(\n",
    "        S.st_qtox(\"q\", <cell size>),\n",
    "        S.st_rtoy(\"r\", <cell size>),\n",
    "        \"count\",\n",
    "    )\n",
    "    .select(\n",
    "        S.st_cell(\"x\", \"y\", <cell size>).alias(\"geometry\"),\n",
    "        F.least(\"count\", F.lit(<max count>)).alias(\"count\"),\n",
    "    )\n",
    "    .orderBy(\"count\")\n",
    ")\n",
    "\n",
    "# this part should be used only when users ask for showing the result in a map\n",
    "df = df.toPandas()\n",
    "df.geometry = df.geometry.apply(lambda _: bytes(_))\n",
    "df.geometry = gp.GeoSeries.from_wkb(df.geometry)\n",
    "gdf = gp.GeoDataFrame(df, crs=\"EPSG:3857\")\n",
    "gdf.explore()\n",
    "\"\"\"\n",
    "\n",
    "GEOBINS_PLOT_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"columns\", \"explain\", \"instruction\"], template=GEOBINS_PLOT_PROMPT_TEMPLATE\n",
    ")\n",
    "\n",
    "DATA_ANALYSIS_PROMPT_TEMPLATE = \"\"\"\n",
    "You are Data Analyst who specializes data analysis. You will give detailed description for each \n",
    "data columns with given sample dataset. \n",
    "\n",
    "{instruction}\n",
    "\"\"\"\n",
    "\n",
    "DATA_ANALYSIS_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"instruction\"], template=DATA_ANALYSIS_PROMPT_TEMPLATE\n",
    ")\n",
    "\n",
    "TOOL_SELECTION_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert in selecting right tool for the given instruction. Here is \n",
    "a list of candidate tools. \n",
    "\n",
    "\"transform_df\": applies a transformation to a provided PySpark DataFrame,\n",
    "the specifics of which are determined by the 'desc' parameter. \n",
    "\n",
    "\"create_geobins\": performs geospatial binning function to create an aggregated data in geojson document. \n",
    "\n",
    "\"plot_df_geobins\": performs geospatial binning function and then plot the data in a live map. \n",
    "\n",
    "\"analyze_s3_dataset\": performs preliminary data analysis on a dataset from Amazon S3 data store. \n",
    "\n",
    "You must select only one of these tools! \n",
    "\n",
    "{instruction}\n",
    "\"\"\"\n",
    "\n",
    "TOOL_SELECTION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"instruction\"], template=TOOL_SELECTION_PROMPT_TEMPLATE\n",
    ")\n",
    "\n",
    "CREATE_GEOBINS_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an Apache Spark SQL expert programmer with knowledge in spatial operations.\n",
    "It is forbidden to include old deprecated APIs in your code.\n",
    "\n",
    "Given a pyspark DataFrame `df`,with the output columns:\n",
    "{columns}\n",
    "\n",
    "And an explanation of `df`: {explain}\n",
    "\n",
    "Write Python code to perform spatial binning and output a GeoJSON document to a local disk.\n",
    "Run help on the following custom library geofunctions S to understand how to use it.\n",
    "There is no need to install any package with pip. \n",
    "Must include any necessary import statements.\n",
    "{instruction}\n",
    "\n",
    "Extract the following variables from the instructions and use them in your sample code below:\n",
    "1. <lon column> (the name of the longitude column)\n",
    "2. <lat column> (the name of the latitude column)\n",
    "3. <cell size> (the size of the spatial bin cell)\n",
    "4. <max count> (the maximum count of the spatial bin cell)\n",
    "\n",
    "\n",
    "sample code:\n",
    "\n",
    "import geofunctions as S\n",
    "import pyspark.sql.functions as F\n",
    "import geopandas as gp\n",
    "\n",
    "#Perform Spatial Binning\n",
    "df = (\n",
    "    df.select(S.st_lontoq(\"<lon column>\", cell), S.st_lattor(\"<lat column>\", <cell size>))\n",
    "    .groupBy(\"q\", \"r\")\n",
    "    .count()\n",
    "    .select(\n",
    "        S.st_qtox(\"q\", <cell size>),\n",
    "        S.st_rtoy(\"r\", <cell size>),\n",
    "        \"count\",\n",
    "    )\n",
    "    .select(\n",
    "        S.st_cell(\"x\", \"y\", <cell size>).alias(\"geometry\"),\n",
    "        F.least(\"count\", F.lit(<max count>)).alias(\"count\"),\n",
    "    )\n",
    "    .orderBy(\"count\")\n",
    ")\n",
    "\n",
    "# create geodataframe to get Geo_JSON document\n",
    "df = df.toPandas()\n",
    "df.geometry = df.geometry.apply(lambda _: bytes(_))\n",
    "df.geometry = gp.GeoSeries.from_wkb(df.geometry)\n",
    "gdf = gp.GeoDataFrame(df, crs=\"EPSG:3857\")\n",
    "geo_json = gdf.to_json()\n",
    "with open('../output/geobins_json.txt', 'w') as f:\n",
    "    f.write(geo_json)\n",
    "\"\"\"\n",
    "\n",
    "CREATE_GEOBINS_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"columns\", \"explain\", \"instruction\"], template=CREATE_GEOBINS_PROMPT_TEMPLATE\n",
    ")\n",
    "\n",
    "VERIFY_TEMPLATE = \"\"\"\n",
    "Given 1) a PySpark dataframe, df, and 2) a description of expected properties, desc,\n",
    "generate a Python function to test whether the given dataframe satisfies the expected properties.\n",
    "Your generated function should take 1 parameter, df, and the return type should be a boolean.\n",
    "You will call the function, passing in df as the parameter, and return the output (True/False).\n",
    "\n",
    "In total, your output must follow the format below, exactly (no explanation words):\n",
    "1. function definition f, in Python (Do NOT surround the function definition with quotes)\n",
    "2. 1 blank new line\n",
    "3. Call f on df and assign the result to a variable, result: result = name_of_f(df)\n",
    "The answer MUST contain python code only. For example, do NOT include \"Here is your output:\"\n",
    "\n",
    "Include any necessary import statements INSIDE the function definition, like this:\n",
    "def gen_random():\n",
    "    import random\n",
    "    return random.randint(0, 10)\n",
    "\n",
    "Your output must follow the format of the example below, exactly:\n",
    "Input:\n",
    "df = DataFrame[name: string, age: int]\n",
    "desc = \"expect 5 columns\"\n",
    "\n",
    "Output:\n",
    "def has_5_columns(df) -> bool:\n",
    "    # Get the number of columns in the DataFrame\n",
    "    num_columns = len(df.columns)\n",
    "\n",
    "    # Check if the number of columns is equal to 5\n",
    "    if num_columns == 5:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "result = has_5_columns(df)\n",
    "\n",
    "No explanation words (e.g. do not say anything like \"Here is your output:\")\n",
    "\n",
    "Here is your input df: {df}\n",
    "Here is your input description: {desc}\n",
    "\"\"\"\n",
    "\n",
    "VERIFY_PROMPT = PromptTemplate(input_variables=[\"df\", \"desc\"], template=VERIFY_TEMPLATE)\n",
    "\n",
    "UDF_PREFIX = \"\"\"\n",
    "This is the documentation for a PySpark user-defined function (udf): pyspark.sql.functions.udf\n",
    "\n",
    "A udf creates a deterministic, reusable function in Spark. It can take any data type as a parameter,\n",
    "and by default returns a String (although it can return any data type).\n",
    "The point is to reuse a function on several dataframes and SQL functions.\n",
    "\n",
    "Given 1) input arguments, 2) a description of the udf functionality,\n",
    "3) the udf return type, and 4) the udf function name,\n",
    "generate and return a callable udf.\n",
    "\n",
    "Return ONLY the callable resulting udf function (no explanation words).\n",
    "Include any necessary import statements INSIDE the function definition.\n",
    "For example:\n",
    "def gen_random():\n",
    "    import random\n",
    "    return random.randint(0, 10)\n",
    "\"\"\"\n",
    "\n",
    "UDF_SUFFIX = \"\"\"\n",
    "input_args_types: {input_args_types}\n",
    "input_desc: {desc}\n",
    "return_type: {return_type}\n",
    "udf_name: {udf_name}\n",
    "output:\\n\n",
    "\"\"\"\n",
    "\n",
    "_udf_output1 = \"\"\"\n",
    "def to_upper(s) -> str:\n",
    "    if s is not None:\n",
    "        return s.upper()\n",
    "\"\"\"\n",
    "\n",
    "_udf_output2 = \"\"\"\n",
    "def add_one(x) -> int:\n",
    "    if x is not None:\n",
    "        return x + 1\n",
    "\"\"\"\n",
    "\n",
    "_udf_examples = [\n",
    "    {\n",
    "        \"input_args_types\": \"(s: str)\",\n",
    "        \"desc\": \"Convert string s to uppercase\",\n",
    "        \"return_type\": \"str\",\n",
    "        \"udf_name\": \"to_upper\",\n",
    "        \"output\": _udf_output1,\n",
    "    },\n",
    "    {\n",
    "        \"input_args_types\": \"(x: int)\",\n",
    "        \"desc\": \"Add 1\",\n",
    "        \"return_type\": \"int\",\n",
    "        \"udf_name\": \"add_one\",\n",
    "        \"output\": _udf_output2,\n",
    "    },\n",
    "]\n",
    "\n",
    "_udf_formatter = \"\"\"\n",
    "input_args_types: {input_args_types}\n",
    "desc: {desc}\n",
    "return_type: {return_type}\n",
    "udf_name: {udf_name}\n",
    "output: {output}\n",
    "\"\"\"\n",
    "\n",
    "_udf_prompt = PromptTemplate(\n",
    "    input_variables=[\"input_args_types\", \"desc\", \"return_type\", \"udf_name\", \"output\"],\n",
    "    template=_udf_formatter,\n",
    ")\n",
    "\n",
    "UDF_PROMPT = FewShotPromptTemplate(\n",
    "    examples=_udf_examples,\n",
    "    example_prompt=_udf_prompt,\n",
    "    prefix=UDF_PREFIX,\n",
    "    suffix=UDF_SUFFIX,\n",
    "    input_variables=[\"input_args_types\", \"desc\", \"return_type\", \"udf_name\"],\n",
    "    example_separator=\"\\n\\n\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd29b5f9",
   "metadata": {},
   "source": [
    "## GeoLLM - Pyspark AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd38479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "from typing import Callable, List, Optional\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pandas as pd  # noqa: F401\n",
    "import requests\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup\n",
    "from pyspark.sql.functions import col, unix_timestamp, from_unixtime\n",
    "\n",
    "AIUtils\n",
    "Cache\n",
    "CodeLogger\n",
    "SKIP_CACHE_TAGS, LLMChainWithCache\n",
    "(\n",
    "    EXPLAIN_DF_PROMPT,\n",
    "    PLOT_PROMPT,\n",
    "    SEARCH_PROMPT,\n",
    "    SQL_PROMPT,\n",
    "    TRANSFORM_PROMPT,\n",
    "    UDF_PROMPT,\n",
    "    VERIFY_PROMPT,\n",
    ")\n",
    "SearchToolWithCache\n",
    "random_view_name, replace_view_name\n",
    "\n",
    "\n",
    "class GeoLLM:\n",
    "    _HTTP_HEADER = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\"\n",
    "        \" (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: Optional[BaseLanguageModel] = None,\n",
    "        web_search_tool: Optional[Callable[[str], str]] = None,\n",
    "        spark_session: Optional[SparkSession] = None,\n",
    "        enable_cache: bool = True,\n",
    "        cache_file_format: str = \"json\",\n",
    "        cache_file_location: Optional[str] = None,\n",
    "        encoding: Optional[Encoding] = None,\n",
    "        max_tokens_of_web_content: int = 3000,\n",
    "        verbose: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the SparkAI object with the provided parameters.\n",
    "\n",
    "        :param llm: LLM instance for selecting web search result\n",
    "                                 and writing the ingestion SQL query.\n",
    "        :param web_search_tool: optional function to perform web search,\n",
    "                                Google search will be used if not provided\n",
    "        :param spark_session: optional SparkSession, a new one will be created if not provided\n",
    "        :param encoding: optional Encoding, cl100k_base will be used if not provided\n",
    "        :param max_tokens_of_web_content: maximum tokens of web content after encoding\n",
    "        \"\"\"\n",
    "        self._spark = spark_session or SparkSession.builder.getOrCreate()\n",
    "        if llm is None:\n",
    "            llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "        self._llm = llm\n",
    "        self._web_search_tool = web_search_tool or self._default_web_search_tool\n",
    "        if enable_cache:\n",
    "            self._enable_cache = enable_cache\n",
    "            if cache_file_location is not None:\n",
    "                # if there is parameter setting for it, use the parameter\n",
    "                self._cache_file_location = cache_file_location\n",
    "            elif \"AI_CACHE_FILE_LOCATION\" in os.environ:\n",
    "                # otherwise read from env variable AI_CACHE_FILE_LOCATION\n",
    "                self._cache_file_location = os.environ[\"AI_CACHE_FILE_LOCATION\"]\n",
    "            else:\n",
    "                # use default value \"spark_ai_cache.json\"\n",
    "                self._cache_file_location = \"spark_ai_cache.json\"\n",
    "            self._cache = Cache(\n",
    "                cache_file_location=self._cache_file_location,\n",
    "                file_format=cache_file_format,\n",
    "            )\n",
    "            self._web_search_tool = SearchToolWithCache(\n",
    "                self._web_search_tool, self._cache\n",
    "            ).search\n",
    "        else:\n",
    "            self._cache = None\n",
    "            self._enable_cache = False\n",
    "        self._encoding = encoding or tiktoken.get_encoding(\"cl100k_base\")\n",
    "        self._max_tokens_of_web_content = max_tokens_of_web_content\n",
    "        self._search_llm_chain = self._create_llm_chain(prompt=SEARCH_PROMPT)\n",
    "        self._sql_llm_chain = self._create_llm_chain(prompt=SQL_PROMPT)\n",
    "        self._data_analysis_llm_chain = self._create_llm_chain(prompt=DATA_ANALYSIS_PROMPT)\n",
    "        self._tool_selection_llm_chain = self._create_llm_chain(prompt=TOOL_SELECTION_PROMPT)\n",
    "        self._explain_chain = self._create_llm_chain(prompt=EXPLAIN_DF_PROMPT)\n",
    "        self._transform_chain = self._create_llm_chain(prompt=TRANSFORM_PROMPT)\n",
    "        self._plot_chain = self._create_llm_chain(prompt=PLOT_PROMPT)\n",
    "        self._geobins_plot_chain = self._create_llm_chain(prompt=GEOBINS_PLOT_PROMPT)\n",
    "        self._create_geobins_chain = self._create_llm_chain(prompt=CREATE_GEOBINS_PROMPT)\n",
    "        self._verify_chain = self._create_llm_chain(prompt=VERIFY_PROMPT)\n",
    "        self._udf_chain = self._create_llm_chain(prompt=UDF_PROMPT)\n",
    "        self._verbose = verbose\n",
    "        if verbose:\n",
    "            self._logger = CodeLogger(\"spark_ai\")\n",
    "\n",
    "    def _create_llm_chain(self, prompt: BasePromptTemplate):\n",
    "        if self._cache is None:\n",
    "            return LLMChain(llm=self._llm, prompt=prompt)\n",
    "\n",
    "        return LLMChainWithCache(llm=self._llm, prompt=prompt, cache=self._cache)\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_view_name(query: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the view name from the provided SQL query.\n",
    "\n",
    "        :param query: SQL query as a string\n",
    "        :return: view name as a string\n",
    "        \"\"\"\n",
    "        pattern = r\"^CREATE(?: OR REPLACE)? TEMP VIEW (\\S+)\"\n",
    "        match = re.search(pattern, query, re.IGNORECASE)\n",
    "        if not match:\n",
    "            raise ValueError(\n",
    "                f\"The provided query: '{query}' is not valid for creating a temporary view. \"\n",
    "                \"Expected pattern: 'CREATE TEMP VIEW [VIEW_NAME] ...'\"\n",
    "            )\n",
    "        return match.group(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_search_prompt(columns: Optional[List[str]]) -> str:\n",
    "        return (\n",
    "            f\"The best search results should contain as many as possible of these info: {','.join(columns)}\"\n",
    "            if columns is not None and len(columns) > 0\n",
    "            else \"\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_sql_prompt(columns: Optional[List[str]]) -> str:\n",
    "        return (\n",
    "            f\"The result view MUST contain following columns: {columns}\"\n",
    "            if columns is not None and len(columns) > 0\n",
    "            else \"\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _default_web_search_tool(desc: str) -> str:\n",
    "        search_wrapper = GoogleSearchAPIWrapper()\n",
    "        return str(search_wrapper.results(query=desc, num_results=10))\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_http_or_https_url(s: str):\n",
    "        result = urlparse(s)  # Parse the URL\n",
    "        # Check if the scheme is 'http' or 'https'\n",
    "        return result.scheme in [\"http\", \"https\"]\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_code_blocks(text) -> List[str]:\n",
    "        code_block_pattern = re.compile(r\"```(.*?)```\", re.DOTALL)\n",
    "        code_blocks = re.findall(code_block_pattern, text)\n",
    "        if code_blocks:\n",
    "            # If there are code blocks, strip them and remove language\n",
    "            # specifiers.\n",
    "            extracted_blocks = []\n",
    "            for block in code_blocks:\n",
    "                block = block.strip()\n",
    "                if block.startswith(\"python\"):\n",
    "                    block = block.replace(\"python\\n\", \"\", 1)\n",
    "                elif block.startswith(\"sql\"):\n",
    "                    block = block.replace(\"sql\\n\", \"\", 1)\n",
    "                extracted_blocks.append(block)\n",
    "            return extracted_blocks\n",
    "        else:\n",
    "            # If there are no code blocks, treat the whole text as a single\n",
    "            # block of code.\n",
    "            return [text]\n",
    "\n",
    "    def log(self, message: str) -> None:\n",
    "        if self._verbose:\n",
    "            self._logger.log(message)\n",
    "\n",
    "    def _trim_text_from_end(self, text: str, max_tokens: int) -> str:\n",
    "        \"\"\"\n",
    "        Trim text from the end based on the maximum number of tokens allowed.\n",
    "\n",
    "        :param text: text to trim\n",
    "        :param max_tokens: maximum tokens allowed\n",
    "        :return: trimmed text\n",
    "        \"\"\"\n",
    "        tokens = list(self._encoding.encode(text))\n",
    "        if len(tokens) > max_tokens:\n",
    "            tokens = tokens[:max_tokens]\n",
    "        return self._encoding.decode(tokens)\n",
    "\n",
    "    def _get_url_from_search_tool(\n",
    "        self, desc: str, columns: Optional[List[str]], cache: bool\n",
    "    ) -> str:\n",
    "        search_result = self._web_search_tool(desc)\n",
    "        search_columns_hint = self._generate_search_prompt(columns)\n",
    "        # Run the LLM chain to pick the best search result\n",
    "        tags = self._get_tags(cache)\n",
    "        return self._search_llm_chain.run(\n",
    "            tags=tags,\n",
    "            query=desc,\n",
    "            search_results=search_result,\n",
    "            columns={search_columns_hint},\n",
    "        )\n",
    "\n",
    "    def _create_dataframe_with_llm(\n",
    "        self, text: str, desc: str, columns: Optional[List[str]], cache: bool\n",
    "    ) -> DataFrame:\n",
    "        clean_text = \" \".join(text.split())\n",
    "        web_content = self._trim_text_from_end(\n",
    "            clean_text, self._max_tokens_of_web_content\n",
    "        )\n",
    "\n",
    "        sql_columns_hint = self._generate_sql_prompt(columns)\n",
    "\n",
    "        # Run the LLM chain to get an ingestion SQL query\n",
    "        tags = self._get_tags(cache)\n",
    "        temp_view_name = random_view_name()\n",
    "        llm_result = self._sql_llm_chain.run(\n",
    "            tags=tags,\n",
    "            query=desc,\n",
    "            web_content=web_content,\n",
    "            view_name=temp_view_name,\n",
    "            columns=sql_columns_hint,\n",
    "        )\n",
    "        sql_query = self._extract_code_blocks(llm_result)[0]\n",
    "        # The actual view name used in the SQL query may be different from the\n",
    "        # temp view name because of caching.\n",
    "        view_name = self._extract_view_name(sql_query)\n",
    "        formatted_sql_query = CodeLogger.colorize_code(sql_query, \"sql\")\n",
    "        self.log(f\"SQL query for the ingestion:\\n{formatted_sql_query}\")\n",
    "        self.log(f\"Storing data into temp view: {view_name}\\n\")\n",
    "        self._spark.sql(sql_query)\n",
    "        return self._spark.table(view_name)    \n",
    "\n",
    "    def _get_df_schema(self, df: DataFrame) -> str:\n",
    "        return \"\\n\".join([f\"{name}: {dtype}\" for name, dtype in df.dtypes])\n",
    "\n",
    "    @staticmethod\n",
    "    def _trim_hash_id(analyzed_plan):\n",
    "        # Pattern to find strings like #59 or #2021\n",
    "        pattern = r\"#\\d+\"\n",
    "\n",
    "        # Remove matching patterns\n",
    "        trimmed_plan = re.sub(pattern, \"\", analyzed_plan)\n",
    "\n",
    "        return trimmed_plan\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_explain_string(df: DataFrame) -> str:\n",
    "        \"\"\"\n",
    "        Helper function to parse the content of the extended explain\n",
    "        string to extract the analyzed logical plan. As Spark does not provide\n",
    "        access to the logical plane without accessing the query execution object\n",
    "        directly, the value is extracted from the explain text representation.\n",
    "\n",
    "        :param df: The dataframe to extract the logical plan from.\n",
    "        :return: The analyzed logical plan.\n",
    "        \"\"\"\n",
    "        with contextlib.redirect_stdout(io.StringIO()) as f:\n",
    "            df.explain(extended=True)\n",
    "        explain = f.getvalue()\n",
    "        splits = explain.split(\"\\n\")\n",
    "        # The two index operations will fail if Spark changes the textual\n",
    "        # plan representation.\n",
    "        begin = splits.index(\"== Analyzed Logical Plan ==\")\n",
    "        end = splits.index(\"== Optimized Logical Plan ==\")\n",
    "        # The analyzed logical plan starts two lines after the section marker.\n",
    "        # The first line is the output schema.\n",
    "        return \"\\n\".join(splits[begin + 2 : end])\n",
    "\n",
    "    def _get_df_explain(self, df: DataFrame, cache: bool) -> str:\n",
    "        raw_analyzed_str = self._parse_explain_string(df)\n",
    "        tags = self._get_tags(cache)\n",
    "        return self._explain_chain.run(\n",
    "            tags=tags, input=self._trim_hash_id(raw_analyzed_str)\n",
    "        )\n",
    "\n",
    "    def _get_tags(self, cache: bool) -> Optional[List[str]]:\n",
    "        if self._enable_cache and not cache:\n",
    "            return SKIP_CACHE_TAGS\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _getColumnInfoJSON(colNameTypeList) -> str:\n",
    "        fieldInfoJson = ''\n",
    "        for col in colNameTypeList:\n",
    "            nameField = '\"name\":\"{}\",'\n",
    "            typeField = '\"type\":\"{}\"'\n",
    "            col_name = nameField.format(col[0])\n",
    "            col_type = typeField.format(col[1])\n",
    "            fieldInfoJson = f'{fieldInfoJson}{{{col_name}{col_type}}},'\n",
    "        return f'[{fieldInfoJson[:-1]}]'\n",
    "\n",
    "    @staticmethod\n",
    "    def _getSampleDataJSON(sampleDataList) -> str:\n",
    "        samples = ''\n",
    "        for idx in range(len(sampleDataList)):\n",
    "            samples = f'{samples}{json.dumps(sampleDataList[0].asDict())},'\n",
    "        return f'[{samples[:-1]}]'\n",
    "    \n",
    "    def create_df(\n",
    "        self, desc: str, columns: Optional[List[str]] = None, cache: bool = True\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Create a Spark DataFrame by querying an LLM from web search result.\n",
    "\n",
    "        :param desc: the description of the result DataFrame, which will be used for\n",
    "                     web searching\n",
    "        :param columns: the expected column names in the result DataFrame\n",
    "        :param cache: If `True`, fetches cached data, if available. If `False`, retrieves fresh data and updates cache.\n",
    "\n",
    "        :return: a Spark DataFrame\n",
    "        \"\"\"\n",
    "        url = desc.strip()  # Remove leading and trailing whitespace\n",
    "        is_url = self._is_http_or_https_url(url)\n",
    "        # If the input is not a valid URL, use search tool to get the dataset.\n",
    "        if not is_url:\n",
    "            url = self._get_url_from_search_tool(desc, columns, cache)\n",
    "\n",
    "        self.log(f\"Parsing URL: {url}\\n\")\n",
    "        try:\n",
    "            response = requests.get(url, headers=self._HTTP_HEADER)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as http_err:\n",
    "            self.log(f\"HTTP error occurred: {http_err}\")\n",
    "            return\n",
    "        except Exception as err:\n",
    "            self.log(f\"Other error occurred: {err}\")\n",
    "            return\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # add url and page content to cache\n",
    "        if cache:\n",
    "            if self._cache.lookup(key=url):\n",
    "                page_content = self._cache.lookup(key=url)\n",
    "            else:\n",
    "                page_content = soup.get_text()\n",
    "                self._cache.update(key=url, val=page_content)\n",
    "        else:\n",
    "            page_content = soup.get_text()\n",
    "\n",
    "        # If the input is a URL link, use the title of web page as the\n",
    "        # dataset's description.\n",
    "        if is_url:\n",
    "            desc = soup.title.string\n",
    "        return self._create_dataframe_with_llm(page_content, desc, columns, cache)\n",
    "\n",
    "    def select_tool(self, desc: str, cache: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Select a tool from a list of candidates.\n",
    "\n",
    "        :param desc: desc for a candidate tool. \n",
    "\n",
    "        \"\"\"\n",
    "        instruction = f\"The purpose of the request: {desc}\" if desc is not None else \"\"\n",
    "        tags = self._get_tags(cache)\n",
    "        tool_list = [\"transform_df\", \"create_geobins\", \"plot_df_geobins\", \"analyze_s3_dataset\"]\n",
    "\n",
    "        response = None\n",
    "        loop_count = 0\n",
    "\n",
    "        while (response is None and loop_count < 3):\n",
    "            response = self._tool_selection_llm_chain.run(\n",
    "                tags=tags,\n",
    "                instruction=instruction,\n",
    "            )\n",
    "            loop_count += 1\n",
    "            \n",
    "        if response is not None:\n",
    "            for tool in tool_list:\n",
    "                if response.find(tool) != -1:\n",
    "                    return tool\n",
    "            response = None # try again\n",
    "        else:\n",
    "            raise Exception(\"Could not find a tool fitting the description\", \"\")\n",
    "            \n",
    "    def analyze_sample_data(self, data_samples: str, cache: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Analyze the data items based on the given sample dataset.\n",
    "\n",
    "        :param desc: a few data samples. \n",
    "\n",
    "        \"\"\"\n",
    "        instruction = f\"The purpose of the request: {data_samples}\" if data_samples is not None else \"\"\n",
    "        tags = self._get_tags(cache)\n",
    "\n",
    "        response = None\n",
    "        loop_count = 0\n",
    "\n",
    "        while (response is None and loop_count < 3):\n",
    "            response = self._data_analysis_llm_chain.run(\n",
    "                tags=tags,\n",
    "                instruction=instruction,\n",
    "            )\n",
    "            loop_count += 1\n",
    "            \n",
    "        if response is not None:\n",
    "            return os.linesep.join([s for s in response.splitlines() if s]).replace(\"\\n\", \"\\\\\\\\n\")\n",
    "        else:\n",
    "            return response\n",
    "\n",
    "    def create_s3_df(self, s3_url, header: bool = True, inferSchema: bool = True, \n",
    "                           number_samples: int = 3, cache: bool = True\n",
    "                          ) -> DataFrame:\n",
    "        if s3_url is None or not s3_url.startswith(\"s3a://\"):\n",
    "            return None \n",
    "\n",
    "        try:\n",
    "            df = self._spark\\\n",
    "                    .read.format(\"csv\")\\\n",
    "                    .option(\"header\", header)\\\n",
    "                    .option(\"inferSchema\", inferSchema)\\\n",
    "                    .load(s3_url)\n",
    "            # replace spaces in the column name with  '_'\n",
    "            return df.select([col(c).alias(c.replace(' ', '_')) for c in df.columns]) \n",
    "        except Exception as e:\n",
    "            return None        \n",
    "    \n",
    "    def analyze_s3_dataset(self, df: DataFrame, number_samples: int = 5, cache: bool = True) -> str:\n",
    "        colInfoJson = self._getColumnInfoJSON(df.dtypes)\n",
    "        samplesJson = self._getSampleDataJSON(df.take(number_samples))\n",
    "        \n",
    "        response = self.analyze_sample_data(samplesJson)\n",
    "        success = True\n",
    "        error_msg = ''\n",
    "        if response is None:\n",
    "            success = False\n",
    "            response = ''\n",
    "            error_msg = \"Failed to get preliminary analysis results from LLM after 3 tries.\"\n",
    "        \n",
    "        return f'{{\"success\":\"{success}\",\"error_msg\":\"{error_msg}\",\"col_data_types\":{colInfoJson},\"samples\":{samplesJson},\"data_descriptions\":\"{response}\"}}'\n",
    "    \n",
    "    def transform_df(self, df: DataFrame, desc: str, cache: bool = True) -> DataFrame:\n",
    "        \"\"\"\n",
    "        This method applies a transformation to a provided Spark DataFrame,\n",
    "        the specifics of which are determined by the 'desc' parameter.\n",
    "\n",
    "        :param df: The Spark DataFrame that is to be transformed.\n",
    "        :param desc: A natural language string that outlines the specific transformation to be applied on the DataFrame.\n",
    "        :param cache: If `True`, fetches cached data, if available. If `False`, retrieves fresh data and updates cache.\n",
    "\n",
    "        :return: Returns a new Spark DataFrame that is the result of applying the specified transformation\n",
    "                 on the input DataFrame.\n",
    "        \"\"\"\n",
    "        # get first record as an example and append to the 'desc'\n",
    "        desc = f'{desc}. Sample: {df.take(1)}.'\n",
    "        \n",
    "        temp_view_name = random_view_name()\n",
    "        create_temp_view_code = CodeLogger.colorize_code(\n",
    "            f'df.createOrReplaceTempView(\"{temp_view_name}\")', \"python\"\n",
    "        )\n",
    "        self.log(f\"Creating temp view for the transform:\\n{create_temp_view_code}\")\n",
    "        df.createOrReplaceTempView(temp_view_name)\n",
    "        schema_str = self._get_df_schema(df)\n",
    "        tags = self._get_tags(cache)\n",
    "        llm_result = self._transform_chain.run(\n",
    "            tags=tags, view_name=temp_view_name, columns=schema_str, desc=desc\n",
    "        )\n",
    "        sql_query_from_response = self._extract_code_blocks(llm_result)[0]\n",
    "        # Replace the temp view name in case the view name is from the cache.\n",
    "        sql_query = replace_view_name(sql_query_from_response, temp_view_name)\n",
    "        formatted_sql_query = CodeLogger.colorize_code(sql_query, \"sql\")\n",
    "        self.log(f\"SQL query for the transform:\\n{formatted_sql_query}\")\n",
    "        return self._spark.sql(sql_query)\n",
    "\n",
    "    def explain_df(self, df: DataFrame, cache: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        This method generates a natural language explanation of the SQL plan of the input Spark DataFrame.\n",
    "\n",
    "        :param df: The Spark DataFrame to be explained.\n",
    "        :param cache: If `True`, fetches cached data, if available. If `False`, retrieves fresh data and updates cache.\n",
    "\n",
    "        :return: A string explanation of the DataFrame's SQL plan, detailing what the DataFrame is intended to retrieve.\n",
    "        \"\"\"\n",
    "        explain_result = self._get_df_explain(df, cache)\n",
    "        # If there is code block in the explain result, ignore it.\n",
    "        if \"```\" in explain_result:\n",
    "            summary = explain_result.split(\"```\")[-1]\n",
    "            return summary.strip()\n",
    "        else:\n",
    "            return explain_result\n",
    "\n",
    "    def plot_df(\n",
    "        self, df: DataFrame, desc: Optional[str] = None, cache: bool = True\n",
    "    ) -> None:\n",
    "        instruction = f\"The purpose of the plot: {desc}\" if desc is not None else \"\"\n",
    "        tags = self._get_tags(cache)\n",
    "        response = self._plot_chain.run(\n",
    "            tags=tags,\n",
    "            columns=self._get_df_schema(df),\n",
    "            explain=self._get_df_explain(df, cache),\n",
    "            instruction=instruction,\n",
    "        )\n",
    "        self.log(response)\n",
    "        codeblocks = self._extract_code_blocks(response)\n",
    "        code = \"\\n\".join(codeblocks)\n",
    "        try:\n",
    "            exec(compile(code, \"plot_df-CodeGen\", \"exec\"))\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Could not evaluate Python code\", e)\n",
    "        \n",
    "    def plot_df_geobins(\n",
    "        self, df: DataFrame, desc: Optional[str] = None, cache: bool = True\n",
    "    ) -> None:\n",
    "        instruction = f\"The purpose of the plot: {desc}\" if desc is not None else \"\"\n",
    "        tags = self._get_tags(cache)\n",
    "\n",
    "        response = None\n",
    "        loop_count = 0\n",
    "\n",
    "        while (response is None and loop_count < 3):\n",
    "            loop_count += 1\n",
    "            response = self._geobins_plot_chain.run(\n",
    "                tags=tags,\n",
    "                columns=self._get_df_schema(df),\n",
    "                explain=self._get_df_explain(df, cache),\n",
    "                instruction=instruction,\n",
    "            )\n",
    "            self.log(response)\n",
    "            codeblocks = self._extract_code_blocks(response)\n",
    "            code = \"\\n\".join(codeblocks)\n",
    "            try:\n",
    "                exec(compile(code, \"plot_df_geobins-CodeGen\", \"exec\"))\n",
    "            except Exception as e:\n",
    "                if loop_count < 3:\n",
    "                    response = None\n",
    "                else:\n",
    "                    raise Exception(\"Could not evaluate Python code\", e)\n",
    "\n",
    "    def create_geobins(\n",
    "        self, df:DataFrame, desc: Optional[str] = None, cache: bool = True\n",
    "    ) -> None:\n",
    "        # assert DataFrame is not None\n",
    "        if df is None:\n",
    "            raise Exception(\"Could not perform geobinning!\", \"The required PySpark DataFrame is not defined!\")\n",
    "        \n",
    "        instruction = f\"The purpose of the plot: {desc}\" if desc is not None else \"\"\n",
    "        tags = self._get_tags(cache)\n",
    "\n",
    "        response = None\n",
    "        loop_count = 0\n",
    "\n",
    "        while (response is None and loop_count < 3):\n",
    "            loop_count += 1\n",
    "            response = self._create_geobins_chain.run(\n",
    "                tags=tags,\n",
    "                columns=self._get_df_schema(df),\n",
    "                explain=self._get_df_explain(df, cache),\n",
    "                instruction=instruction,\n",
    "            )\n",
    "            self.log(response)\n",
    "            codeblocks = self._extract_code_blocks(response)\n",
    "            code = \"\\n\".join(codeblocks)\n",
    "            try:\n",
    "                exec(compile(code, \"create_geobins-CodeGen\", \"exec\"))\n",
    "            except Exception as e:\n",
    "                self.log(code)\n",
    "                if loop_count < 3:\n",
    "                    response = None\n",
    "                else:\n",
    "                    raise Exception(\"Could not evaluate Python code in create_geobins after 3 tries!\", e)\n",
    "\n",
    "    def verify_df(self, df: DataFrame, desc: str, cache: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        This method creates and runs test cases for the provided PySpark dataframe transformation function.\n",
    "\n",
    "        :param df: The Spark DataFrame to be verified\n",
    "        :param desc: A description of the expectation to be verified\n",
    "        :param cache: If `True`, fetches cached data, if available. If `False`, retrieves fresh data and updates cache.\n",
    "        \"\"\"\n",
    "        tags = self._get_tags(cache)\n",
    "        llm_output = self._verify_chain.run(tags=tags, df=df, desc=desc)\n",
    "\n",
    "        codeblocks = self._extract_code_blocks(llm_output)\n",
    "        llm_output = \"\\n\".join(codeblocks)\n",
    "\n",
    "        self.log(f\"LLM Output:\\n{llm_output}\")\n",
    "\n",
    "        formatted_code = CodeLogger.colorize_code(llm_output, \"python\")\n",
    "        self.log(f\"Generated code:\\n{formatted_code}\")\n",
    "\n",
    "        locals_ = {}\n",
    "        try:\n",
    "            exec(compile(llm_output, \"verify_df-CodeGen\", \"exec\"), {\"df\": df}, locals_)\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Could not evaluate Python code\", e)\n",
    "        self.log(f\"\\nResult: {locals_['result']}\")\n",
    "\n",
    "    def udf(self, func: Callable) -> Callable:\n",
    "        from inspect import signature\n",
    "\n",
    "        desc = func.__doc__\n",
    "        func_signature = str(signature(func))\n",
    "        input_args_types = func_signature.split(\"->\")[0].strip()\n",
    "        return_type = func_signature.split(\"->\")[1].strip()\n",
    "        udf_name = func.__name__\n",
    "\n",
    "        code = self._udf_chain.run(\n",
    "            input_args_types=input_args_types,\n",
    "            desc=desc,\n",
    "            return_type=return_type,\n",
    "            udf_name=udf_name,\n",
    "        )\n",
    "\n",
    "        formatted_code = CodeLogger.colorize_code(code, \"python\")\n",
    "        self.log(f\"Creating following Python UDF:\\n{formatted_code}\")\n",
    "\n",
    "        locals_ = {}\n",
    "        try:\n",
    "            exec(compile(code, \"udf-CodeGen\", \"exec\"), globals(), locals_)\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Could not evaluate Python code\", e)\n",
    "        return locals_[udf_name]\n",
    "\n",
    "    def activate(self):\n",
    "        \"\"\"\n",
    "        Activates AI utility functions for Spark DataFrame.\n",
    "        \"\"\"\n",
    "        DataFrame.ai = AIUtils(self)\n",
    "        # Patch the Spark Connect DataFrame as well.\n",
    "        try:\n",
    "            from pyspark.sql.connect.dataframe import DataFrame as CDataFrame\n",
    "\n",
    "            CDataFrame.ai = AIUtils(self)\n",
    "        except ImportError:\n",
    "            self.log(\n",
    "                \"The pyspark.sql.connect.dataframe module could not be imported. \"\n",
    "                \"This might be due to your PySpark version being below 3.4.\"\n",
    "            )\n",
    "\n",
    "    def commit(self):\n",
    "        \"\"\"\n",
    "        Commit the staging in-memory cache into persistent cache, if cache is enabled.\n",
    "        \"\"\"\n",
    "        if self._cache is not None:\n",
    "            self._cache.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dc0e80-7631-48d6-a8dc-35be352d50b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "class GeoAgent:\n",
    "\n",
    "    def __init__(self, geollm: GeoLLM):\n",
    "        self._geollm = geollm\n",
    "        self._tools = [\"transform_df\", \"create_geobins\", \"plot_df_geobins\", \"analyze_s3_dataset\"]\n",
    "\n",
    "    def _extract_s3_url(self, desc: str) -> str:\n",
    "        # Regular expression pattern for S3 URL extraction\n",
    "        pattern = r's3a://[a-zA-Z0-9\\-\\.]+/[a-zA-Z0-9\\-_\\/\\.]+'\n",
    "        # Extract the S3 URL\n",
    "        s3_url = re.search(pattern, desc)\n",
    "        if s3_url:\n",
    "            s3_url = s3_url.group(0)\n",
    "            if s3_url[-1] in '.!@#$%^&*()':\n",
    "                s3_url = s3_url[:-1]\n",
    "        else:\n",
    "            s3_url = None\n",
    "\n",
    "        return s3_url\n",
    "    \n",
    "    def _quick_check_transformed_df(self, df: DataFrame) -> bool:\n",
    "        # compare heads records of 2 DataFrames \n",
    "        head1 = self._df.take(1)[0]\n",
    "        head2 = df.take(1)[0]\n",
    "        for name, dtype in df.dtypes:\n",
    "            if head1[name] is not None and head2[name] is None:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def chat(self, message: str) -> str:\n",
    "        tool = self._geollm.select_tool(message)\n",
    "        print(tool)\n",
    "        match tool:\n",
    "            case 'analyze_s3_dataset':\n",
    "                s3_url = self._extract_s3_url(message)\n",
    "                if s3_url is None:\n",
    "                    return f'{{\"success\":\"False\", \"error_msg\":\"Required s3 url is invalid: {s3_url}\"}}'\n",
    "                else:\n",
    "                    print(s3_url)\n",
    "                    df = self._geollm.create_s3_df(s3_url)\n",
    "                    if df is None:\n",
    "                        return f'{{\"success\":\"False\", \"error_msg\":\"Failed to load CSV data from the given url: {s3_url}\"}}'\n",
    "                    else:\n",
    "                        self._df = df\n",
    "                        return self._geollm.analyze_s3_dataset(df)\n",
    "            \n",
    "            case 'transform_df':\n",
    "                try:\n",
    "                    loop = 0\n",
    "                    quick_check = False\n",
    "                    while not quick_check and loop < 3:\n",
    "                        loop += 1\n",
    "                        df = self._geollm.transform_df(self._df, message)\n",
    "                        quick_check = self._quick_check_transformed_df(df)\n",
    "                        \n",
    "                    if quick_check:\n",
    "                        self._df = df\n",
    "                        return f'{{\"success\": \"True\", \"error_msg\":\"\"}}'\n",
    "                    else:\n",
    "                        return f'{{\"success\": \"False\", \"error_msg\":\"Could not transform the data. Please refine your message and try again!\"}}'    \n",
    "                except Exception as e:\n",
    "                    return f'{{\"success\": \"False\", \"error_msg\":\"Could not transform the data: {e}. Please refine your message and try again!\"}}'  \n",
    "            \n",
    "            case 'create_geobins':\n",
    "                try:\n",
    "                    self._geollm.create_geobins(self._df, message) \n",
    "                    file_name = '../output/geobins_json.txt'\n",
    "                    if not os.path.isfile(file_name):\n",
    "                        file_name = './geobins_json.txt'\n",
    "                        if not os.path.isfile(file_name):\n",
    "                            return f'{{\"success\": \"False\", \"error_msg\":\"geobinning data may have failed. Please refine your message and try again!\"}}' \n",
    "                            \n",
    "                    with open(file_name, 'r') as f:\n",
    "                        geojson = f.read()\n",
    "                        print(len(geojson))\n",
    "                        print(geojson[0:500])\n",
    "                        return geojson\n",
    "    \n",
    "                except Exception as e:\n",
    "                    return f'{{\"success\": \"False\", \"error_msg\":\"Could not perform geobinning the data: {e}. Please refine your message and try again!\"}}'  \n",
    "                \n",
    "            case _:\n",
    "                return f'{{\"success\": \"False\", \"error_msg\":\"Could not find a tool matching your description. Please refine your message and try again!\"}}'\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f25de7",
   "metadata": {},
   "source": [
    "## Running Miami Geodatabase Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3042e07f-8e21-4bac-b4b5-cfd344be85fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_jars = glob.glob(os.path.join(\"..\", \"lib\", \"*.jar\"))[0]\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"SpatialBin\")\n",
    "    .config(\"spark.driver.memory\", \"64G\")\n",
    "    .config(\"spark.executor.memory\", \"64G\")\n",
    "    .config(\"spark.memory.offHeap.enabled\", True)\n",
    "    .config(\"spark.memory.offHeap.size\", \"64G\")\n",
    "    .config(\"spark.ui.enabled\", False)\n",
    "    .config(\"spark.jars\", spark_jars)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa852b5e-a8a7-4462-983c-3fee9fabf150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7507691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.appName(\"GeoLLM\").getOrCreate()\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_base=\"https://azure-openai-personal-stylist.openai.azure.com/\",\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    openai_api_key=\"d6b01920a9f64098994de5dc830b8a94\",\n",
    "    openai_api_type=\"azure\",\n",
    ")\n",
    "\n",
    "geollm = GeoLLM(llm, spark_session=spark.getActiveSession(), enable_cache=False, verbose=True)\n",
    "geollm.activate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5633ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_path = os.path.join(\"..\", \"data\", \"Miami.gdb\")\n",
    "\n",
    "df = (\n",
    "    spark.read.format(\"gdb\")\n",
    "    .options(path=gdb_path, name=\"Broadcast\")\n",
    "    .load()\n",
    "    .filter(\"Status == 0\")\n",
    "    .select(\n",
    "        F.col(\"Shape.x\").alias(\"lon\"),\n",
    "        F.col(\"shape.y\").alias(\"lat\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8709d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is no guarantee that the output geojson will be written to the 'output' folder!  \n",
    "# geollm.create_geobins(\n",
    "#     df, \n",
    "#     \"\"\"\n",
    "#     apply a spatial binning on the dataframe using lon column and lat column and cell size is 300 and \n",
    "#     max cell count is 70\n",
    "#     \"\"\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cc850a-2f8e-4a76-b5ac-de3b6b4b3d67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb76fa16-5cea-4e2e-bf7f-54722ecd35ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In some case, LLM does not generate correct code. Here is an code response I got on 08/21/2023.\n",
    "# In this case, we need to re-run the LLM request for code generation! \n",
    "# \n",
    "# INFO: Note: As an AI language model, I do not have the ability to access external files or libraries such as the geofunctions library mentioned in the instructions. However, I can provide a sample code based on the given instructions that can be adapted to use a relevant library for spatial binning.\n",
    "\n",
    "# import pyspark.sql.functions as F\n",
    "# import geopandas as gp\n",
    "\n",
    "# #Perform Spatial Binning using geopandas\n",
    "# cell_size = 300\n",
    "# max_count = 70\n",
    "# df = (\n",
    "#     df.groupby(\n",
    "#         gp.GeoSeries(\n",
    "#             {\n",
    "#                 \"geometry\": gp.points_from_xy(df[\"lon\"], df[\"lat\"]),\n",
    "#                 \"crs\": \"EPSG:4326\",\n",
    "#             }\n",
    "#         ).to_crs(epsg=3857).apply(lambda p: (int(p.x / cell_size), int(p.y / cell_size)))\n",
    "#     )\n",
    "#     .size()\n",
    "#     .to_frame(name=\"count\")\n",
    "#     .reset_index()\n",
    "#     .assign(\n",
    "#         geometry=lambda x: gp.GeoSeries(\n",
    "#             {\n",
    "#                 \"type\": \"Polygon\",\n",
    "#                 \"coordinates\": [\n",
    "#                     [\n",
    "#                         [x[\"level_0\"] * cell_size, x[\"level_1\"] * cell_size],\n",
    "#                         [(x[\"level_0\"] + 1) * cell_size, x[\"level_1\"] * cell_size],\n",
    "#                         [(x[\"level_0\"] + 1) * cell_size, (x[\"level_1\"] + 1) * cell_size],\n",
    "#                         [x[\"level_0\"] * cell_size, (x[\"level_1\"] + 1) * cell_size],\n",
    "#                         [x[\"level_0\"] * cell_size, x[\"level_1\"] * cell_size],\n",
    "#                     ]\n",
    "#                 ],\n",
    "#             }\n",
    "#         )\n",
    "#     )\n",
    "#     .sort_values(by=\"count\", ascending=False)\n",
    "#     .head(max_count)\n",
    "# )\n",
    "\n",
    "# # create GeoJSON document\n",
    "# geo_json = df.to_json()\n",
    "# with open('../output/geobins_json.txt', 'w') as f:\n",
    "#     f.write(geo_json)\n",
    "\n",
    "#  \n",
    "# With  following exception: \n",
    "# Exception: ('Could not evaluate Python code', SyntaxError('invalid syntax', ('create_geobins-CodeGen', 1, 10, 'Note: As an AI language model, I do not have the ability to access external files or libraries such as the geofunctions library mentioned in the instructions. However, I can provide a sample code based on the given instructions that can be adapted to use a relevant library for spatial binning.\\n', 1, 12)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b17cad9-1326-4ec2-89bd-a1f30ea4696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../output/geobins_json.txt', 'r') as f:\n",
    "#     geojson = f.read()\n",
    "#     print(len(geojson))\n",
    "#     print(geojson[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cced0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geollm.plot_df_geobins(\n",
    "#     df, \n",
    "#     \"\"\"\n",
    "#     apply a spatial binning on the dataframe using lon column and lat column and cell size is 300 and \n",
    "#     max cell count is 70\n",
    "#     \"\"\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd65ac-7d8f-4a88-a81a-f42e7da0d39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# static plot works\n",
    "# \n",
    "# with following prompt template - show second part only \n",
    "# \n",
    "\"\"\"\n",
    "sample code:\n",
    "\n",
    "import geofunctions as S\n",
    "import pyspark.sql.functions as F\n",
    "import geopandas as gp\n",
    "\n",
    "#Perform Spatial Binning\n",
    "df = (\n",
    "    df.select(S.st_lontoq(\"<lon column>\", cell), S.st_lattor(\"<lat column>\", <cell size>))\n",
    "    .groupBy(\"q\", \"r\")\n",
    "    .count()\n",
    "    .select(\n",
    "        S.st_qtox(\"q\", <cell size>),\n",
    "        S.st_rtoy(\"r\", <cell size>),\n",
    "        \"count\",\n",
    "    )\n",
    "    .select(\n",
    "        S.st_cell(\"x\", \"y\", <cell size>).alias(\"geometry\"),\n",
    "        F.least(\"count\", F.lit(<max count>)).alias(\"count\"),\n",
    "    )\n",
    "    .orderBy(\"count\")\n",
    ")\n",
    "\n",
    "# this part should be used only when users ask for showing the result in a map\n",
    "df = df.toPandas()\n",
    "df.geometry = df.geometry.apply(lambda _: bytes(_))\n",
    "df.geometry = gp.GeoSeries.from_wkb(df.geometry)\n",
    "gdf = gp.GeoDataFrame(df, crs=\"EPSG:3857\")\n",
    "gdf.sindex\n",
    "gdf.plot(\"count\")\n",
    "\"\"\"\n",
    "# import geofunctions as S\n",
    "# import pyspark.sql.functions as F\n",
    "# import geopandas as gp\n",
    "\n",
    "# # Define variables\n",
    "# lon_col = \"lon\"\n",
    "# lat_col = \"lat\"\n",
    "# cell_size = 300\n",
    "# max_count = 70\n",
    "\n",
    "# # Perform Spatial Binning\n",
    "# df = (\n",
    "#     df.select(S.st_lontoq(lon_col, cell_size), S.st_lattor(lat_col, cell_size))\n",
    "#     .groupBy(\"q\", \"r\")\n",
    "#     .count()\n",
    "#     .select(\n",
    "#         S.st_qtox(\"q\", cell_size),\n",
    "#         S.st_rtoy(\"r\", cell_size),\n",
    "#         \"count\",\n",
    "#     )\n",
    "#     .select(\n",
    "#         S.st_cell(\"x\", \"y\", cell_size).alias(\"geometry\"),\n",
    "#         F.least(\"count\", F.lit(max_count)).alias(\"count\"),\n",
    "#     )\n",
    "#     .orderBy(\"count\")\n",
    "# )\n",
    "\n",
    "# # Convert to GeoDataFrame and plot\n",
    "# df = df.toPandas()\n",
    "# df.geometry = df.geometry.apply(lambda _: bytes(_))\n",
    "# df.geometry = gp.GeoSeries.from_wkb(df.geometry)\n",
    "# gdf = gp.GeoDataFrame(df, crs=\"EPSG:3857\")\n",
    "# gdf.plot(column=\"count\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b58710c-678f-4abf-8de1-26099ba1d15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bda842bf-6ee7-432c-8f57-5a964c46f3e0",
   "metadata": {},
   "source": [
    "## Chicago Crime Dataset (2001 - 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9c0e1f-b93e-417c-9928-0095d1886f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These two environment variables should be set during System start up \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a88445-28cc-44c3-ba40-79d4439238b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0 - Pre-config Spark in each user session \n",
    "spark_jars = glob.glob(os.path.join(\"..\", \"lib\", \"*.jar\"))[0]\n",
    "\n",
    "aws_access_key = os.environ[\"AWS_ACCESS_KEY\"]\n",
    "aws_secret_key = os.environ[\"AWS_SECRET_KEY\"]\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"SpatialBin\")\n",
    "    .config(\"spark.driver.memory\", \"64G\")\n",
    "    .config(\"spark.executor.memory\", \"64G\")\n",
    "    .config(\"spark.memory.offHeap.enabled\", True)\n",
    "    .config(\"spark.memory.offHeap.size\", \"64G\")\n",
    "    .config(\"spark.ui.enabled\", False)\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", aws_access_key)\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret_key)\n",
    "    .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    "    .config(\"spark.jars\", spark_jars)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf75c97520bf43a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    openai_api_base=\"https://azure-openai-personal-stylist.openai.azure.com/\",\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    openai_api_key=\"d6b01920a9f64098994de5dc830b8a94\",\n",
    "    openai_api_type=\"azure\",\n",
    ")\n",
    "\n",
    "geollm = GeoLLM(llm, spark_session=spark.getActiveSession(), enable_cache=False, verbose=True)\n",
    "geollm.activate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c4248-d0ea-40d1-8135-6e6020720e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = GeoAgent(geollm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0760607b-074b-4d96-b94d-4b36f82d5a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "message1 = \"\"\" Here is my AWS S3 connection string: \"s3a://frankxia-s3/chicago_crime_data/Crimes_-_2023.csv\".!@#, \n",
    "               please return some meta data for the given data source. \"\"\"\n",
    "response = agent.chat(message1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741be568-d924-4e42-8843-cba062ca9848",
   "metadata": {},
   "outputs": [],
   "source": [
    "message2 = \"\"\" transform a string 'Date' column to a timestamp column. \"\"\"\n",
    "response2 = agent.chat(message2)\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f262750-1d3a-49f4-98b0-ae3c40d7e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "message3 = \"\"\" use 'Date' column to select data between \"2023-01-01\" and \"2023-04-30\". \"\"\"\n",
    "response3 = agent.chat(message3)\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87baeb5d-dc68-4201-9342-781ed692a16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "message4 = \"\"\" \n",
    "apply a spatial binning on the dataframe using Longitude column and Latitude column and cell size is 100 and max cell count is 70. \n",
    "\"\"\"\n",
    "response4 = agent.chat(message4)\n",
    "print(response4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be1445f-ca02-40c7-a223-e54001943875",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../output/geobins_json.txt', 'r') as f:\n",
    "    geojson = f.read()\n",
    "    print(len(geojson))\n",
    "    print(geojson[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5daf07-92c1-4930-a9df-431fc59c2727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a690a619-0f20-430a-a2de-a59b5624a558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ad562-1be7-417f-a5e2-f524e550d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - User post a request with S3 URL  \n",
    "# The system will respond with a preliminary analysis in JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb89ff7-d294-47e0-9b5b-7391ea228823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_instruction = \"\"\"\n",
    "#                     Here is my AWS S3 connection string: \"s3a://frankxia-s3/chicago_crime_data/Crimes_-_2023.csv\".!@#, \n",
    "#                     please return some meta data for the given data source.\n",
    "#                     \"\"\"\n",
    "# tool = geollm.select_tool(user_instruction)\n",
    "# print(tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb623f-0053-42d8-a781-0005941e71f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_url = extract_s3_url(user_instruction)\n",
    "# if s3_url is None:\n",
    "#     response = f'{{\"success\":\"False\", \"error_msg\":\"s3 url is invalid: {s3_url}\"}}'\n",
    "# print(s3_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597a3370-613c-4cdb-95b6-ee99b59ebd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = geollm.create_s3_df(s3_url)\n",
    "# if df is None:\n",
    "#     response = f'{{\"success\":\"False\", \"error_msg\":\"Failed to load CSV data from the given url: {s3_url}\"}}'\n",
    "# else:\n",
    "#     response = geollm.analyze_s3_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d196b5c1-2477-472b-9eb7-74a1fdb50637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb1e140-5748-4f7f-b8d2-3c03e3f748e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Filter the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00fbd2f-3463-4be1-b7dc-9fc2ebc9e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ebc27e-b01f-4673-91d0-6c229c112d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_message =  \"\"\" transform a string 'Date' column to a timestamp column. \"\"\"\n",
    "# tool = geollm.select_tool(user_message)\n",
    "# print(tool)\n",
    "# df1 = geollm.transform_df(df,user_message) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc69510-ae59-4995-8d58-953dd2757b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287e1b61-6c2b-4027-bfe3-dc8d7b250348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.count())\n",
    "# print(df1.count())\n",
    "# df1.take(3)\n",
    "# how can we make sure the generated SQL works as expected? We may need to check the result! \n",
    "# \n",
    "# in above example, the generated code works but occasionally it will generate code that wouldn't as exected such as \n",
    "# \n",
    "# SELECT ID, Case_Number, CAST(TO_TIMESTAMP(Date, 'MM/dd/yyyy hh:mm:ss aa') AS TIMESTAMP) AS Date, Block, IUCR, Primary_Type, Description, Location_Description, Arrest, Domestic, Beat, District, Ward, Community_Area, FBI_Code, X_Coordinate, Y_Coordinate, Year, Updated_On, Latitude, Longitude, Location\n",
    "# FROM spark_ai_temp_view_1657e4\n",
    "# \n",
    "# where the Date time format string, 'MM/dd/yyyy hh:mm:ss aa' is WRONG, it will result in an exception from PySpark due to the extra in the end! \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b8e6db-55a5-481e-a485-e741d304ed87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aaca55-2e8f-4c64-b5ec-92bbb0039814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_msg =  \"\"\" use 'Date' column to select data between \"2023-01-01\" and \"2023-04-30\". \"\"\"\n",
    "# tool = geollm.select_tool( filter_msg)\n",
    "# print(tool)\n",
    "# df2 = geollm.transform_df(df1,filter_msg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba8f499-b244-4706-91d9-3822623b5847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(df2.dtypes)\n",
    "# print(df2.count())\n",
    "# df2.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9325a445-010e-4556-872e-666e751efcd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497f420b-e092-4e5b-a920-61bcd545e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - create geobins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2a7132-a2dd-4934-8a85-1763faceaa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_geobins_msg =   \"\"\"\n",
    "# apply a spatial binning on the dataframe using Longitude column and Latitude column and cell size is 100 and max cell count is 70\n",
    "# \"\"\"\n",
    "# tool = geollm.select_tool( create_geobins_msg )\n",
    "# print(tool)\n",
    "# geollm.create_geobins(df2, create_geobins_msg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a112fe53-894f-427a-b27f-07baa88c40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../output/geobins_json.txt', 'r') as f:\n",
    "#     geojson = f.read()\n",
    "#     print(len(geojson))\n",
    "#     print(geojson[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958c1613-0c6e-4268-9255-e19b241c2e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cbc38a-b5eb-4ac6-a4a6-de4f10d6f773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b164c945-28a0-4348-a391-d5a14a3af58f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
