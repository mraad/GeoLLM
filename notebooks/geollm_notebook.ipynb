{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f89968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "from typing import Callable\n",
    "from urllib.parse import urlparse\n",
    "import pandas\n",
    "import requests\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain import BasePromptTemplate, GoogleSearchAPIWrapper, LLMChain\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "from langchain.chat_models import ChatOpenAI, AzureChatOpenAI\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from tiktoken import Encoding\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7109cb2",
   "metadata": {},
   "source": [
    "## AI Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7103a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Type\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "class AIMethodWrapper:\n",
    "    \"\"\"\n",
    "    This class wraps the AI utility functions to allow them to be used directly\n",
    "    on DataFrame instances. An instance of this class is created each time the\n",
    "    utility functions are accessed, with the DataFrame and SparkAI instance\n",
    "    passed to it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, spark_ai, df_instance: DataFrame):\n",
    "        \"\"\"\n",
    "        Initialize the AIMethodWrapper with the given SparkAI and DataFrame instance.\n",
    "\n",
    "        Args:\n",
    "            spark_ai: The SparkAI instance containing the AI utility methods.\n",
    "            df_instance: The DataFrame instance on which the utility methods will be used.\n",
    "        \"\"\"\n",
    "        self.spark_ai = spark_ai\n",
    "        self.df_instance = df_instance\n",
    "\n",
    "    def transform(self, desc: str, cache: bool = True) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Transform the DataFrame using the given description.\n",
    "\n",
    "        Args:\n",
    "            desc: A string description specifying the transformation.\n",
    "            cache: Indicates whether to utilize a cache for this method.\n",
    "                If `True`, fetches cached data, if available.\n",
    "                If `False`, retrieves fresh data and updates cache.\n",
    "\n",
    "        Returns:\n",
    "            The transformed DataFrame.\n",
    "        \"\"\"\n",
    "        return self.spark_ai.transform_df(self.df_instance, desc, cache)\n",
    "\n",
    "    def explain(self, cache: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Explain the DataFrame.\n",
    "\n",
    "        Args:\n",
    "            cache: Indicates whether to utilize a cache for this method.\n",
    "                If `True`, fetches cached data, if available.\n",
    "                If `False`, retrieves fresh data and updates cache.\n",
    "\n",
    "        Returns:\n",
    "            A string explanation of the DataFrame.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.spark_ai.explain_df(self.df_instance, cache)\n",
    "\n",
    "    def plot(self, desc: Optional[str] = None, cache: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Plot the DataFrame.\n",
    "\n",
    "        Args:\n",
    "            desc: A string description specifying the plot.\n",
    "            cache: Indicates whether to utilize a cache for this method.\n",
    "                If `True`, fetches cached data, if available.\n",
    "                If `False`, retrieves fresh data and updates cache.\n",
    "        \"\"\"\n",
    "        return self.spark_ai.plot_df(self.df_instance, desc, cache)\n",
    "\n",
    "    def verify(self, desc: str, cache: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Verify the DataFrame using the given description.\n",
    "\n",
    "        Args:\n",
    "            desc: A string description specifying what to verify in the DataFrame.\n",
    "            cache: Indicates whether to utilize a cache for this method.\n",
    "                If `True`, fetches cached data, if available.\n",
    "                If `False`, retrieves fresh data and updates cache.\n",
    "        \"\"\"\n",
    "        return self.spark_ai.verify_df(self.df_instance, desc, cache)\n",
    "\n",
    "\n",
    "class AIUtils:\n",
    "    \"\"\"\n",
    "    This class is a descriptor that is used to add AI utility methods to DataFrame instances.\n",
    "    When the utility methods are accessed, it returns a new AIMethodWrapper instance with the\n",
    "    DataFrame and SparkAI instance passed to it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, spark_ai):\n",
    "        \"\"\"\n",
    "        Initialize the AIUtils descriptor with the given SparkAI.\n",
    "\n",
    "        Args:\n",
    "            spark_ai: The SparkAI instance containing the AI utility methods.\n",
    "        \"\"\"\n",
    "        self.spark_ai = spark_ai\n",
    "\n",
    "    def __get__(self, instance: DataFrame, owner: Type[DataFrame]) -> AIMethodWrapper:\n",
    "        \"\"\"\n",
    "        This method is called when the AI utility methods are accessed on a DataFrame instance.\n",
    "        It returns a new AIMethodWrapper instance with the DataFrame instance and SparkAI passed to it.\n",
    "\n",
    "        Args:\n",
    "            instance: The DataFrame instance on which the utility methods are being accessed.\n",
    "            owner: The class (DataFrame) to which this descriptor is added.\n",
    "\n",
    "        Returns:\n",
    "            A new AIMethodWrapper instance.\n",
    "        \"\"\"\n",
    "        return AIMethodWrapper(self.spark_ai, instance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d494aed8",
   "metadata": {},
   "source": [
    "## File Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31af0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "from langchain.cache import SQLiteCache\n",
    "from langchain.schema import Generation\n",
    "\n",
    "\n",
    "class FileCache(ABC):\n",
    "    \"\"\"Base interface for a file-based cache.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def lookup(self, key: str) -> Optional[str]:\n",
    "        \"\"\"Perform a lookup based on the key.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, key: str, val: str) -> None:\n",
    "        \"\"\"Update cache based on the key.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def clear(self, **kwargs: Any) -> None:\n",
    "        \"\"\"Clear cache. Can take additional keyword arguments.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def commit_staging_cache(self, staging_cache: Dict[str, str]) -> None:\n",
    "        \"\"\"Commit all items from the staging_cache to the cache.\"\"\"\n",
    "\n",
    "\n",
    "class SQLiteCacheWrapper(FileCache):\n",
    "    \"\"\"Wrapper class for SQLiteCache that ignores llm_string during lookups and updates.\"\"\"\n",
    "\n",
    "    def __init__(self, cache_file_location: str):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the SQLiteCacheWrapper class.\n",
    "\n",
    "        Args:\n",
    "            cache_file_location (str): The SQLite file location\n",
    "        \"\"\"\n",
    "        self._sqlite_cache = SQLiteCache(database_path=cache_file_location)\n",
    "\n",
    "    def lookup(self, key: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Performs a lookup in the SQLiteCache using the given key.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key string for the lookup.\n",
    "\n",
    "        Returns:\n",
    "            Optional[RETURN_VAL_TYPE]: The cached value corresponding to the key, if available. Otherwise, None.\n",
    "        \"\"\"\n",
    "        lookup_result = self._sqlite_cache.lookup(prompt=key, llm_string=\"\")\n",
    "        if lookup_result is not None and len(lookup_result) > 0:\n",
    "            return lookup_result[0].text\n",
    "        return None\n",
    "\n",
    "    def update(self, key: str, val: str) -> None:\n",
    "        \"\"\"\n",
    "        Updates the SQLiteCache with the given key and return value.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key string.\n",
    "            val (RETURN_VAL_TYPE): The return value to be cached.\n",
    "        \"\"\"\n",
    "        stored_value = [Generation(text=val)]\n",
    "        self._sqlite_cache.update(key, \"\", stored_value)\n",
    "\n",
    "    def clear(self, **kwargs: Any) -> None:\n",
    "        \"\"\"\n",
    "        Clears the SQLiteCache.\n",
    "\n",
    "        Args:\n",
    "            **kwargs: Additional keyword arguments for the clear method of SQLiteCache.\n",
    "        \"\"\"\n",
    "        self._sqlite_cache.clear(**kwargs)\n",
    "\n",
    "    def commit_staging_cache(self, staging_cache: Dict[str, str]) -> None:\n",
    "        \"\"\"\n",
    "        Commits all items from the staging_cache to the SQLiteCache.\n",
    "\n",
    "        Args:\n",
    "            staging_cache (Dict[str, str]): The staging cache to be committed.\n",
    "        \"\"\"\n",
    "        for key, value in staging_cache.items():\n",
    "            self.update(key, value)\n",
    "\n",
    "\n",
    "class JsonCache(FileCache):\n",
    "    \"\"\"A simple caching system using a JSON file for storage, subclass of FileCache.\"\"\"\n",
    "\n",
    "    def __init__(self, filepath: str):\n",
    "        \"\"\"Initialize a new JsonCache instance.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): The path to the JSON file to use for the cache.\n",
    "        \"\"\"\n",
    "        self.filepath = filepath\n",
    "        # If cache file exists, load it into memory.\n",
    "        self.cache = {}\n",
    "        if os.path.exists(self.filepath):\n",
    "            with open(self.filepath, \"r\") as f:\n",
    "                for line in f:\n",
    "                    if line.strip():  # Avoid empty lines\n",
    "                        line_cache = json.loads(line)\n",
    "                        self.cache[line_cache[\"key\"]] = line_cache[\"value\"]\n",
    "        # Create an empty staging cache for storing changes before they are\n",
    "        # committed.\n",
    "        self.staging_cache: Dict = {}\n",
    "\n",
    "    def update(self, key: str, value: str) -> None:\n",
    "        \"\"\"Store a value in the cache for a given key.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key string.\n",
    "            value (RETURN_VAL_TYPE): The value to store in the cache.\n",
    "        \"\"\"\n",
    "        # Store the value in the staging cache.\n",
    "        self.staging_cache[key] = value\n",
    "\n",
    "    def lookup(self, key: str) -> Optional[str]:\n",
    "        \"\"\"Retrieve a value from the cache for a given key.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key string.\n",
    "\n",
    "        Returns:\n",
    "            Optional[RETURN_VAL_TYPE]: The cached value for the given key, or None if no such value exists.\n",
    "        \"\"\"\n",
    "        return self.cache.get(key)\n",
    "\n",
    "    def commit_staging_cache(self, staging_cache: Dict[str, str]) -> None:\n",
    "        \"\"\"Commit all changes in the staging cache to the cache file.\n",
    "\n",
    "        This method writes all changes in the staging cache to the end of the cache file and then clears\n",
    "        the staging cache.\n",
    "\n",
    "        Args:\n",
    "            staging_cache (Dict[str, str]): The staging cache to be committed.\n",
    "        \"\"\"\n",
    "        # Append the staging cache to the existing cache\n",
    "        self.cache.update(staging_cache)\n",
    "        with open(self.filepath, \"a\") as f:\n",
    "            for key, value in staging_cache.items():\n",
    "                json.dump({\"key\": key, \"value\": value}, f)\n",
    "                f.write(\"\\n\")\n",
    "        # Clear the staging cache\n",
    "        self.staging_cache = {}\n",
    "\n",
    "    def clear(self, **kwargs: Any) -> None:\n",
    "        \"\"\"Clear the cache.\n",
    "\n",
    "        This method removes all entries from the cache and deletes the cache file.\n",
    "        \"\"\"\n",
    "        self.cache = {}\n",
    "        self.staging_cache = {}\n",
    "        os.remove(self.filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6867a5a4",
   "metadata": {},
   "source": [
    "## Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc812e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "\n",
    "JsonCache, SQLiteCacheWrapper, FileCache\n",
    "\n",
    "\n",
    "class Cache:\n",
    "    \"\"\"\n",
    "    This class provides an interface for a simple in-memory and persistent cache system. It keeps an in-memory staging\n",
    "    cache, which gets updated through the `update` method and can be persisted through the `commit` method. Cache\n",
    "    lookup is first performed on the in-memory staging cache, and if not found, it is performed on the persistent\n",
    "    cache.\n",
    "\n",
    "    Attributes:\n",
    "        _staging_updates: A dictionary to keep track of the in-memory staging updates.\n",
    "        _file_cache: An instance of either JsonCache or SQLiteCacheWrapper that acts as the persistent cache.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, cache_file_location: str = \".pyspark_ai.json\", file_format: str = \"json\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the Cache class.\n",
    "\n",
    "        Args:\n",
    "            cache_file_location (str, optional): The path to the cache file for the JsonCache or SQLiteCacheWrapper.\n",
    "                Defaults to \".pyspark_ai.json\".\n",
    "            file_format (str, optional): The format of the file to use for the cache. Defaults to \"json\".\n",
    "        \"\"\"\n",
    "        self._staging_updates: Dict[str, str] = {}\n",
    "        if file_format == \"json\":\n",
    "            self._file_cache: FileCache = JsonCache(cache_file_location)\n",
    "        else:\n",
    "            self._file_cache = SQLiteCacheWrapper(cache_file_location)\n",
    "\n",
    "    def lookup(self, key: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Performs a lookup in the cache using the given key.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key string for the lookup.\n",
    "\n",
    "        Returns:\n",
    "            Optional[str]: The cached text corresponding to the key, if available. Otherwise, None.\n",
    "        \"\"\"\n",
    "        # First look in the staging cache\n",
    "        staging_result = self._staging_updates.get(key)\n",
    "        if staging_result is not None:\n",
    "            return staging_result\n",
    "        # If not found in staging cache, look in the persistent cache\n",
    "        return self._file_cache.lookup(key)\n",
    "\n",
    "    def update(self, key: str, val: str) -> None:\n",
    "        \"\"\"\n",
    "        Updates the staging cache with the given key and value.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key string.\n",
    "            val (str): The value to be cached.\n",
    "        \"\"\"\n",
    "        self._staging_updates[key] = val\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"\n",
    "        Clears both the in-memory staging cache and the persistent cache.\n",
    "        \"\"\"\n",
    "        self._file_cache.clear()\n",
    "        self._staging_updates = {}\n",
    "\n",
    "    def commit(self) -> None:\n",
    "        \"\"\"\n",
    "        Commits all the staged updates to the persistent cache.\n",
    "        \"\"\"\n",
    "        self._file_cache.commit_staging_cache(self._staging_updates)\n",
    "        self._staging_updates = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8102c2",
   "metadata": {},
   "source": [
    "## Code Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315bc7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from pygments import highlight\n",
    "from pygments.formatters import TerminalFormatter\n",
    "from pygments.lexers import PythonLexer, SqlLexer\n",
    "\n",
    "GREEN = \"\\033[92m\"  # terminal code for green\n",
    "RESET = \"\\033[0m\"  # reset terminal color\n",
    "\n",
    "\n",
    "# Custom Formatter\n",
    "class CustomFormatter(logging.Formatter):\n",
    "    def format(self, record):\n",
    "        return GREEN + \"INFO: \" + RESET + super().format(record)\n",
    "\n",
    "\n",
    "class CodeLogger:\n",
    "    def __init__(self, name):\n",
    "        self.logger = logging.getLogger(name)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        if not self.logger.handlers:\n",
    "            handler = logging.StreamHandler(sys.stdout)\n",
    "            handler.setFormatter(\n",
    "                CustomFormatter(\"%(message)s\")\n",
    "            )  # output only the message\n",
    "            self.logger.addHandler(handler)\n",
    "\n",
    "    @staticmethod\n",
    "    def colorize_code(code, language):\n",
    "        if not language or language.lower() == \"python\":\n",
    "            lexer = PythonLexer()\n",
    "        elif language.lower() == \"sql\":\n",
    "            lexer = SqlLexer()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported language: {language}\")\n",
    "        return highlight(code, lexer, TerminalFormatter())\n",
    "\n",
    "    def log(self, message):\n",
    "        # Define pattern to match code blocks with optional language specifiers\n",
    "        pattern = r\"```(python|sql)?(.*?)```\"\n",
    "        # Split message into parts. Every 3rd part will be a code block.\n",
    "        parts = re.split(pattern, message, flags=re.DOTALL)\n",
    "\n",
    "        colored_message = \"\"\n",
    "        for i in range(0, len(parts), 3):\n",
    "            # Add regular text to the message\n",
    "            colored_message += parts[i]\n",
    "            # If there is a code block, colorize it and add it to the message\n",
    "            if i + 2 < len(parts):\n",
    "                colored_message += (\n",
    "                    \"\\n```\\n\" + self.colorize_code(parts[i + 2], parts[i + 1]) + \"```\"\n",
    "                )\n",
    "        # Log the message with colored code blocks\n",
    "        self.logger.info(colored_message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5bc591",
   "metadata": {},
   "source": [
    "## Temp View Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04227b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "\n",
    "prefix = \"spark_ai_temp_view\"\n",
    "pattern = f\"{prefix}_[0-9a-zA-Z]{{6}}\"\n",
    "\n",
    "\n",
    "def random_view_name() -> str:\n",
    "    \"\"\"\n",
    "    Generate a random temp view name.\n",
    "    \"\"\"\n",
    "    return f\"{prefix}_{uuid.uuid4().hex[:6]}\"\n",
    "\n",
    "\n",
    "def canonize_string(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace all occurrences of 'spark_ai_temp_view' followed by 6 alphanumeric characters with 'spark_ai_temp_view'\n",
    "     in a given string.\n",
    "\n",
    "    Args:\n",
    "        s (str): The string in which to replace substrings.\n",
    "\n",
    "    Returns:\n",
    "        str: The modified string with all matching substrings replaced.\n",
    "    \"\"\"\n",
    "    return re.sub(pattern, prefix, s)\n",
    "\n",
    "\n",
    "def replace_view_name(s: str, random_view: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace all the 'spark_ai_temp_view' followed by 6 alphanumeric characters in a given string with a random view\n",
    "     name.\n",
    "    \"\"\"\n",
    "    return re.sub(pattern, random_view, s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c31e65",
   "metadata": {},
   "source": [
    "## LLM Chain with Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce858f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain.callbacks.manager import Callbacks\n",
    "\n",
    "Cache\n",
    "canonize_string\n",
    "\n",
    "SKIP_CACHE_TAGS = [\"SKIP_CACHE\"]\n",
    "\n",
    "\n",
    "class LLMChainWithCache(LLMChain):\n",
    "    cache: Cache\n",
    "\n",
    "    @staticmethod\n",
    "    def _sort_and_stringify(*args: Any) -> str:\n",
    "        # Convert all arguments to strings, then sort them\n",
    "        sorted_args = sorted(str(arg) for arg in args)\n",
    "        # Join all the sorted, stringified arguments with a space\n",
    "        result = \" \".join(sorted_args)\n",
    "        return result\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        *args: Any,\n",
    "        callbacks: Callbacks = None,\n",
    "        tags: Optional[List[str]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        assert not args, \"The chain expected no arguments\"\n",
    "        prompt_str = canonize_string(self.prompt.format_prompt(**kwargs).to_string())\n",
    "        use_cache = tags != SKIP_CACHE_TAGS\n",
    "        cached_result = self.cache.lookup(prompt_str) if use_cache else None\n",
    "        if cached_result is not None:\n",
    "            return cached_result\n",
    "        result = super().run(*args, callbacks=callbacks, tags=tags, **kwargs)\n",
    "        if use_cache:\n",
    "            self.cache.update(prompt_str, result)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1e0624",
   "metadata": {},
   "source": [
    "## Search Tool with Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f271747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "Cache\n",
    "\n",
    "\n",
    "class SearchToolWithCache:\n",
    "    def __init__(self, web_search_tool: Callable[[str], str], cache: Cache):\n",
    "        self.web_search_tool = web_search_tool\n",
    "        self.cache = cache\n",
    "\n",
    "    def search(self, query: str) -> str:\n",
    "        # Try to get the result from the cache\n",
    "        key = f\"web_search:{query}\"\n",
    "        cached_result = self.cache.lookup(key)\n",
    "        if cached_result is not None:\n",
    "            return cached_result\n",
    "\n",
    "        # If the result was not in the cache, use the web_search_tool\n",
    "        result = self.web_search_tool(query)\n",
    "\n",
    "        # Update the cache with the new result\n",
    "        self.cache.update(key, result)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4428d332",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3234d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flake8: noqa\n",
    "from langchain import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "SEARCH_TEMPLATE = \"\"\"Given a Query and a list of Google Search Results, return the link\n",
    "from a reputable website which contains the data set to answer the question. {columns}\n",
    "Query:{query}\n",
    "Google Search Results:\n",
    "```\n",
    "{search_results}\n",
    "```\n",
    "The answer MUST contain the url link only\n",
    "\"\"\"\n",
    "\n",
    "SEARCH_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"search_results\", \"columns\"], template=SEARCH_TEMPLATE\n",
    ")\n",
    "\n",
    "SQL_TEMPLATE = \"\"\"Given the following question:\n",
    "```\n",
    "{query}\n",
    "```\n",
    "I got the following answer from a web page:\n",
    "```\n",
    "{web_content}\n",
    "```\n",
    "Now help me write a SQL query to store the answer into a temp view.\n",
    "Give each column a clearly descriptive name (no abbreviations).\n",
    "If a column can be either String or Numeric, ingest it as Numeric.\n",
    "Here is an example of how to store data into the temp view {view_name}:\n",
    "```\n",
    "CREATE OR REPLACE TEMP VIEW {view_name} AS SELECT * FROM VALUES('Citizen Kane', 1941), ('Schindler\\'s List', 1993) AS v1(title, year)\n",
    "```\n",
    "{columns}\n",
    "The answer MUST contain query only and the temp view MUST be {view_name}.\n",
    "\"\"\"\n",
    "\n",
    "SQL_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"web_content\", \"view_name\", \"columns\"],\n",
    "    template=SQL_TEMPLATE,\n",
    ")\n",
    "\n",
    "TRANSFORM_TEMPLATE = \"\"\"\n",
    "Given a Spark temp view `{view_name}` with the following columns:\n",
    "```\n",
    "{columns}\n",
    "```\n",
    "Write a Spark SQL query to retrieve: {desc}\n",
    "The answer MUST contain query only. Ensure your answer is correct.\n",
    "\"\"\"\n",
    "\n",
    "TRANSFORM_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"view_name\", \"columns\", \"desc\"], template=TRANSFORM_TEMPLATE\n",
    ")\n",
    "\n",
    "EXPLAIN_PREFIX = \"\"\"You are an Apache Spark SQL expert, who can summary what a dataframe retrieves. Given an analyzed\n",
    "query plan of a dataframe, you will\n",
    "1. convert the dataframe to SQL query. Note that an explain output contains plan\n",
    "nodes separated by `\\\\n`. Each plan node has its own expressions and expression ids.\n",
    "2. summary what the sql query retrieves.\n",
    "\"\"\"\n",
    "\n",
    "EXPLAIN_SUFFIX = \"analyzed_plan: {input}\\nexplain:\"\n",
    "\n",
    "_plan1 = \"\"\"\n",
    "GlobalLimit 100\n",
    "    +- LocalLimit 100\n",
    "       +- Sort [d_year ASC NULLS FIRST, sum_agg DESC NULLS LAST, brand_id ASC NULLS FIRST], true\n",
    "          +- Aggregate [d_year, i_brand, i_brand_id], [d_year, i_brand_id AS brand_id, i_brand AS brand, sum(ss_ext_sales_price) AS sum_agg]\n",
    "             +- Filter (((d_date_sk = ss_sold_date_sk) AND (ss_item_sk = i_item_sk)) AND ((i_manufact_id = 128) AND (d_moy = 11)))\n",
    "                +- Join Inner\n",
    "                   :- Join Inner\n",
    "                   :  :- SubqueryAlias dt\n",
    "                   :  :  +- SubqueryAlias spark_catalog.tpcds_sf1_delta.date_dim\n",
    "                   :  :     +- Relation spark_catalog.tpcds_sf1_delta.date_dim[d_date_sk,d_date_id,d_date,d_month_seq,d_week_seq,d_quarter_seq,d_year,d_dow,d_moy,d_dom,d_qoy,d_fy_year,d_fy_quarter_seq,d_fy_week_seq,d_day_name,d_quarter_name,d_holiday,d_weekend,d_following_holiday,d_first_dom,d_last_dom,d_same_day_ly,d_same_day_lq,d_current_day,... 4 more fields] parquet\n",
    "                   :  +- SubqueryAlias spark_catalog.tpcds_sf1_delta.store_sales\n",
    "                   :     +- Relation spark_catalog.tpcds_sf1_delta.store_sales[ss_sold_date_sk,ss_sold_time_sk,ss_item_sk,ss_customer_sk,ss_cdemo_sk,ss_hdemo_sk,ss_addr_sk,ss_store_sk,ss_promo_sk,ss_ticket_numberL,ss_quantity,ss_wholesale_cost,ss_list_price,ss_sales_price,ss_ext_discount_amt,ss_ext_sales_price,ss_ext_wholesale_cost,ss_ext_list_price,ss_ext_tax,ss_coupon_amt,ss_net_paid,ss_net_paid_inc_tax,ss_net_profit] parquet\n",
    "                   +- SubqueryAlias spark_catalog.tpcds_sf1_delta.item\n",
    "                      +- Relation spark_catalog.tpcds_sf1_delta.item[i_item_sk,i_item_id,i_rec_start_date,i_rec_end_date,i_item_desc,i_current_price,i_wholesale_cost,i_brand_id,i_brand,i_class_id,i_class,i_category_id,i_category,i_manufact_id,i_manufact,i_size,i_formulation,i_color,i_units,i_container,i_manager_id,i_product_name] parquet\n",
    "\"\"\"\n",
    "\n",
    "_explain1 = \"\"\"\n",
    "The analyzed plan can be translated into the following SQL query:\n",
    "```sql\n",
    "SELECT\n",
    "  dt.d_year,\n",
    "  item.i_brand_id brand_id,\n",
    "  item.i_brand brand,\n",
    "  SUM(ss_ext_sales_price) sum_agg\n",
    "FROM date_dim dt, store_sales, item\n",
    "WHERE dt.d_date_sk = store_sales.ss_sold_date_sk\n",
    "  AND store_sales.ss_item_sk = item.i_item_sk\n",
    "  AND item.i_manufact_id = 128\n",
    "  AND dt.d_moy = 11\n",
    "GROUP BY dt.d_year, item.i_brand, item.i_brand_id\n",
    "ORDER BY dt.d_year, sum_agg DESC, brand_id\n",
    "LIMIT 100\n",
    "```\n",
    "In summary, this dataframe is retrieving the top 100 brands (specifically of items manufactured by manufacturer with id 128) with the highest total sales price for each year in the month of November. It presents the results sorted by year, total sales (in descending order), and brand id.\n",
    "\"\"\"\n",
    "\n",
    "_explain_examples = [{\"analyzed_plan\": _plan1, \"explain\": _explain1}]\n",
    "\n",
    "_example_formatter = \"\"\"\n",
    "analyzed_plan: {analyzed_plan}\n",
    "explain: {explain}\n",
    "\"\"\"\n",
    "\n",
    "_example_prompt = PromptTemplate(\n",
    "    input_variables=[\"analyzed_plan\", \"explain\"], template=_example_formatter\n",
    ")\n",
    "\n",
    "EXPLAIN_DF_PROMPT = FewShotPromptTemplate(\n",
    "    examples=_explain_examples,\n",
    "    example_prompt=_example_prompt,\n",
    "    prefix=EXPLAIN_PREFIX,\n",
    "    suffix=EXPLAIN_SUFFIX,\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\\n\",\n",
    ")\n",
    "\n",
    "PLOT_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an Apache Spark SQL expert programmer.\n",
    "It is forbidden to include old deprecated APIs in your code.\n",
    "For example, you will not use the pandas method \"append\" because it is deprecated.\n",
    "\n",
    "Given a pyspark DataFrame `df`, with the output columns:\n",
    "{columns}\n",
    "\n",
    "And an explanation of `df`: {explain}\n",
    "\n",
    "Write Python code to visualize the result of `df` using plotly. Make sure to use the exact column names of `df`.\n",
    "Your code may NOT contain \"append\" anywhere. Instead of append, use pd.concat.\n",
    "There is no need to install any package with pip. Do include any necessary import statements.\n",
    "Display the plot directly, instead of saving into an HTML.\n",
    "Do not use scatter plot to display any kind of percentage data.\n",
    "You must import and start your Spark session if you use a Spark DataFrame.\n",
    "Remember to ensure that your code does NOT include \"append\" anywhere, under any circumstance (use pd.concat instead).\n",
    "There is no need to read from a csv file. You can use directly the DataFrame `df` as input.\n",
    "Ensure that your code is correct.\n",
    "{instruction}\n",
    "\"\"\"\n",
    "\n",
    "PLOT_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"columns\", \"explain\", \"instruction\"], template=PLOT_PROMPT_TEMPLATE\n",
    ")\n",
    "\n",
    "GEOBINS_PLOT_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an Apache Spark SQL expert programmer with knowledge in spatial operations.\n",
    "It is forbidden to include old deprecated APIs in your code.\n",
    "\n",
    "Given a pyspark DataFrame `df`,with the output columns:\n",
    "{columns}\n",
    "\n",
    "And an explanation of `df`: {explain}\n",
    "\n",
    "Write Python code to perform spatial binning.\n",
    "Run help on the following custom library geofunctions S to understand how to use it.\n",
    "There is no need to install any package with pip. Do include any necessary import statements.\n",
    "{instruction}\n",
    "\n",
    "Extract the following variables from the instructions and use them in your sample code below:\n",
    "1. <lon column> (the name of the longitude column)\n",
    "2. <lat column> (the name of the latitude column)\n",
    "3. <cell size> (the size of the spatial bin cell)\n",
    "4. <max count> (the maximum count of the spatial bin cell)\n",
    "\n",
    "\n",
    "sample code:\n",
    "\n",
    "import geofunctions as S\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "#Perform Spatial Binning\n",
    "df = (\n",
    "    df.select(S.st_lontoq(\"<lon column>\", cell), S.st_lattor(\"<lat column>\", <cell size>))\n",
    "    .groupBy(\"q\", \"r\")\n",
    "    .count()\n",
    "    .select(\n",
    "        S.st_qtox(\"q\", <cell size>),\n",
    "        S.st_rtoy(\"r\", <cell size>),\n",
    "        \"count\",\n",
    "    )\n",
    "    .select(\n",
    "        S.st_cell(\"x\", \"y\", <cell size>).alias(\"geometry\"),\n",
    "        F.least(\"count\", F.lit(<max count>)).alias(\"count\"),\n",
    "    )\n",
    "    .orderBy(\"count\")\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "GEOBINS_PLOT_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"columns\", \"explain\", \"instruction\"], template=GEOBINS_PLOT_PROMPT_TEMPLATE\n",
    ")\n",
    "\n",
    "VERIFY_TEMPLATE = \"\"\"\n",
    "Given 1) a PySpark dataframe, df, and 2) a description of expected properties, desc,\n",
    "generate a Python function to test whether the given dataframe satisfies the expected properties.\n",
    "Your generated function should take 1 parameter, df, and the return type should be a boolean.\n",
    "You will call the function, passing in df as the parameter, and return the output (True/False).\n",
    "\n",
    "In total, your output must follow the format below, exactly (no explanation words):\n",
    "1. function definition f, in Python (Do NOT surround the function definition with quotes)\n",
    "2. 1 blank new line\n",
    "3. Call f on df and assign the result to a variable, result: result = name_of_f(df)\n",
    "The answer MUST contain python code only. For example, do NOT include \"Here is your output:\"\n",
    "\n",
    "Include any necessary import statements INSIDE the function definition, like this:\n",
    "def gen_random():\n",
    "    import random\n",
    "    return random.randint(0, 10)\n",
    "\n",
    "Your output must follow the format of the example below, exactly:\n",
    "Input:\n",
    "df = DataFrame[name: string, age: int]\n",
    "desc = \"expect 5 columns\"\n",
    "\n",
    "Output:\n",
    "def has_5_columns(df) -> bool:\n",
    "    # Get the number of columns in the DataFrame\n",
    "    num_columns = len(df.columns)\n",
    "\n",
    "    # Check if the number of columns is equal to 5\n",
    "    if num_columns == 5:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "result = has_5_columns(df)\n",
    "\n",
    "No explanation words (e.g. do not say anything like \"Here is your output:\")\n",
    "\n",
    "Here is your input df: {df}\n",
    "Here is your input description: {desc}\n",
    "\"\"\"\n",
    "\n",
    "VERIFY_PROMPT = PromptTemplate(input_variables=[\"df\", \"desc\"], template=VERIFY_TEMPLATE)\n",
    "\n",
    "UDF_PREFIX = \"\"\"\n",
    "This is the documentation for a PySpark user-defined function (udf): pyspark.sql.functions.udf\n",
    "\n",
    "A udf creates a deterministic, reusable function in Spark. It can take any data type as a parameter,\n",
    "and by default returns a String (although it can return any data type).\n",
    "The point is to reuse a function on several dataframes and SQL functions.\n",
    "\n",
    "Given 1) input arguments, 2) a description of the udf functionality,\n",
    "3) the udf return type, and 4) the udf function name,\n",
    "generate and return a callable udf.\n",
    "\n",
    "Return ONLY the callable resulting udf function (no explanation words).\n",
    "Include any necessary import statements INSIDE the function definition.\n",
    "For example:\n",
    "def gen_random():\n",
    "    import random\n",
    "    return random.randint(0, 10)\n",
    "\"\"\"\n",
    "\n",
    "UDF_SUFFIX = \"\"\"\n",
    "input_args_types: {input_args_types}\n",
    "input_desc: {desc}\n",
    "return_type: {return_type}\n",
    "udf_name: {udf_name}\n",
    "output:\\n\n",
    "\"\"\"\n",
    "\n",
    "_udf_output1 = \"\"\"\n",
    "def to_upper(s) -> str:\n",
    "    if s is not None:\n",
    "        return s.upper()\n",
    "\"\"\"\n",
    "\n",
    "_udf_output2 = \"\"\"\n",
    "def add_one(x) -> int:\n",
    "    if x is not None:\n",
    "        return x + 1\n",
    "\"\"\"\n",
    "\n",
    "_udf_examples = [\n",
    "    {\n",
    "        \"input_args_types\": \"(s: str)\",\n",
    "        \"desc\": \"Convert string s to uppercase\",\n",
    "        \"return_type\": \"str\",\n",
    "        \"udf_name\": \"to_upper\",\n",
    "        \"output\": _udf_output1,\n",
    "    },\n",
    "    {\n",
    "        \"input_args_types\": \"(x: int)\",\n",
    "        \"desc\": \"Add 1\",\n",
    "        \"return_type\": \"int\",\n",
    "        \"udf_name\": \"add_one\",\n",
    "        \"output\": _udf_output2,\n",
    "    },\n",
    "]\n",
    "\n",
    "_udf_formatter = \"\"\"\n",
    "input_args_types: {input_args_types}\n",
    "desc: {desc}\n",
    "return_type: {return_type}\n",
    "udf_name: {udf_name}\n",
    "output: {output}\n",
    "\"\"\"\n",
    "\n",
    "_udf_prompt = PromptTemplate(\n",
    "    input_variables=[\"input_args_types\", \"desc\", \"return_type\", \"udf_name\", \"output\"],\n",
    "    template=_udf_formatter,\n",
    ")\n",
    "\n",
    "UDF_PROMPT = FewShotPromptTemplate(\n",
    "    examples=_udf_examples,\n",
    "    example_prompt=_udf_prompt,\n",
    "    prefix=UDF_PREFIX,\n",
    "    suffix=UDF_SUFFIX,\n",
    "    input_variables=[\"input_args_types\", \"desc\", \"return_type\", \"udf_name\"],\n",
    "    example_separator=\"\\n\\n\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd29b5f9",
   "metadata": {},
   "source": [
    "## GeoLLM - Pyspark AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd38479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "from typing import Callable, List, Optional\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pandas as pd  # noqa: F401\n",
    "import requests\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "AIUtils\n",
    "Cache\n",
    "CodeLogger\n",
    "SKIP_CACHE_TAGS, LLMChainWithCache\n",
    "(\n",
    "    EXPLAIN_DF_PROMPT,\n",
    "    PLOT_PROMPT,\n",
    "    SEARCH_PROMPT,\n",
    "    SQL_PROMPT,\n",
    "    TRANSFORM_PROMPT,\n",
    "    UDF_PROMPT,\n",
    "    VERIFY_PROMPT,\n",
    ")\n",
    "SearchToolWithCache\n",
    "random_view_name, replace_view_name\n",
    "\n",
    "\n",
    "class GeoLLM:\n",
    "    _HTTP_HEADER = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\"\n",
    "        \" (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: Optional[BaseLanguageModel] = None,\n",
    "        web_search_tool: Optional[Callable[[str], str]] = None,\n",
    "        spark_session: Optional[SparkSession] = None,\n",
    "        enable_cache: bool = True,\n",
    "        cache_file_format: str = \"json\",\n",
    "        cache_file_location: Optional[str] = None,\n",
    "        encoding: Optional[Encoding] = None,\n",
    "        max_tokens_of_web_content: int = 3000,\n",
    "        verbose: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the SparkAI object with the provided parameters.\n",
    "\n",
    "        :param llm: LLM instance for selecting web search result\n",
    "                                 and writing the ingestion SQL query.\n",
    "        :param web_search_tool: optional function to perform web search,\n",
    "                                Google search will be used if not provided\n",
    "        :param spark_session: optional SparkSession, a new one will be created if not provided\n",
    "        :param encoding: optional Encoding, cl100k_base will be used if not provided\n",
    "        :param max_tokens_of_web_content: maximum tokens of web content after encoding\n",
    "        \"\"\"\n",
    "        self._spark = spark_session or SparkSession.builder.getOrCreate()\n",
    "        if llm is None:\n",
    "            llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "        self._llm = llm\n",
    "        self._web_search_tool = web_search_tool or self._default_web_search_tool\n",
    "        if enable_cache:\n",
    "            self._enable_cache = enable_cache\n",
    "            if cache_file_location is not None:\n",
    "                # if there is parameter setting for it, use the parameter\n",
    "                self._cache_file_location = cache_file_location\n",
    "            elif \"AI_CACHE_FILE_LOCATION\" in os.environ:\n",
    "                # otherwise read from env variable AI_CACHE_FILE_LOCATION\n",
    "                self._cache_file_location = os.environ[\"AI_CACHE_FILE_LOCATION\"]\n",
    "            else:\n",
    "                # use default value \"spark_ai_cache.json\"\n",
    "                self._cache_file_location = \"spark_ai_cache.json\"\n",
    "            self._cache = Cache(\n",
    "                cache_file_location=self._cache_file_location,\n",
    "                file_format=cache_file_format,\n",
    "            )\n",
    "            self._web_search_tool = SearchToolWithCache(\n",
    "                self._web_search_tool, self._cache\n",
    "            ).search\n",
    "        else:\n",
    "            self._cache = None\n",
    "        self._encoding = encoding or tiktoken.get_encoding(\"cl100k_base\")\n",
    "        self._max_tokens_of_web_content = max_tokens_of_web_content\n",
    "        self._search_llm_chain = self._create_llm_chain(prompt=SEARCH_PROMPT)\n",
    "        self._sql_llm_chain = self._create_llm_chain(prompt=SQL_PROMPT)\n",
    "        self._explain_chain = self._create_llm_chain(prompt=EXPLAIN_DF_PROMPT)\n",
    "        self._transform_chain = self._create_llm_chain(prompt=TRANSFORM_PROMPT)\n",
    "        self._plot_chain = self._create_llm_chain(prompt=PLOT_PROMPT)\n",
    "        self._geobins_plot_chain = self._create_llm_chain(prompt=GEOBINS_PLOT_PROMPT)\n",
    "        self._verify_chain = self._create_llm_chain(prompt=VERIFY_PROMPT)\n",
    "        self._udf_chain = self._create_llm_chain(prompt=UDF_PROMPT)\n",
    "        self._verbose = verbose\n",
    "        if verbose:\n",
    "            self._logger = CodeLogger(\"spark_ai\")\n",
    "\n",
    "    def _create_llm_chain(self, prompt: BasePromptTemplate):\n",
    "        if self._cache is None:\n",
    "            return LLMChain(llm=self._llm, prompt=prompt)\n",
    "\n",
    "        return LLMChainWithCache(llm=self._llm, prompt=prompt, cache=self._cache)\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_view_name(query: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the view name from the provided SQL query.\n",
    "\n",
    "        :param query: SQL query as a string\n",
    "        :return: view name as a string\n",
    "        \"\"\"\n",
    "        pattern = r\"^CREATE(?: OR REPLACE)? TEMP VIEW (\\S+)\"\n",
    "        match = re.search(pattern, query, re.IGNORECASE)\n",
    "        if not match:\n",
    "            raise ValueError(\n",
    "                f\"The provided query: '{query}' is not valid for creating a temporary view. \"\n",
    "                \"Expected pattern: 'CREATE TEMP VIEW [VIEW_NAME] ...'\"\n",
    "            )\n",
    "        return match.group(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_search_prompt(columns: Optional[List[str]]) -> str:\n",
    "        return (\n",
    "            f\"The best search results should contain as many as possible of these info: {','.join(columns)}\"\n",
    "            if columns is not None and len(columns) > 0\n",
    "            else \"\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_sql_prompt(columns: Optional[List[str]]) -> str:\n",
    "        return (\n",
    "            f\"The result view MUST contain following columns: {columns}\"\n",
    "            if columns is not None and len(columns) > 0\n",
    "            else \"\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _default_web_search_tool(desc: str) -> str:\n",
    "        search_wrapper = GoogleSearchAPIWrapper()\n",
    "        return str(search_wrapper.results(query=desc, num_results=10))\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_http_or_https_url(s: str):\n",
    "        result = urlparse(s)  # Parse the URL\n",
    "        # Check if the scheme is 'http' or 'https'\n",
    "        return result.scheme in [\"http\", \"https\"]\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_code_blocks(text) -> List[str]:\n",
    "        code_block_pattern = re.compile(r\"```(.*?)```\", re.DOTALL)\n",
    "        code_blocks = re.findall(code_block_pattern, text)\n",
    "        if code_blocks:\n",
    "            # If there are code blocks, strip them and remove language\n",
    "            # specifiers.\n",
    "            extracted_blocks = []\n",
    "            for block in code_blocks:\n",
    "                block = block.strip()\n",
    "                if block.startswith(\"python\"):\n",
    "                    block = block.replace(\"python\\n\", \"\", 1)\n",
    "                elif block.startswith(\"sql\"):\n",
    "                    block = block.replace(\"sql\\n\", \"\", 1)\n",
    "                extracted_blocks.append(block)\n",
    "            return extracted_blocks\n",
    "        else:\n",
    "            # If there are no code blocks, treat the whole text as a single\n",
    "            # block of code.\n",
    "            return [text]\n",
    "\n",
    "    def log(self, message: str) -> None:\n",
    "        if self._verbose:\n",
    "            self._logger.log(message)\n",
    "\n",
    "    def _trim_text_from_end(self, text: str, max_tokens: int) -> str:\n",
    "        \"\"\"\n",
    "        Trim text from the end based on the maximum number of tokens allowed.\n",
    "\n",
    "        :param text: text to trim\n",
    "        :param max_tokens: maximum tokens allowed\n",
    "        :return: trimmed text\n",
    "        \"\"\"\n",
    "        tokens = list(self._encoding.encode(text))\n",
    "        if len(tokens) > max_tokens:\n",
    "            tokens = tokens[:max_tokens]\n",
    "        return self._encoding.decode(tokens)\n",
    "\n",
    "    def _get_url_from_search_tool(\n",
    "        self, desc: str, columns: Optional[List[str]], cache: bool\n",
    "    ) -> str:\n",
    "        search_result = self._web_search_tool(desc)\n",
    "        search_columns_hint = self._generate_search_prompt(columns)\n",
    "        # Run the LLM chain to pick the best search result\n",
    "        tags = self._get_tags(cache)\n",
    "        return self._search_llm_chain.run(\n",
    "            tags=tags,\n",
    "            query=desc,\n",
    "            search_results=search_result,\n",
    "            columns={search_columns_hint},\n",
    "        )\n",
    "\n",
    "    def _create_dataframe_with_llm(\n",
    "        self, text: str, desc: str, columns: Optional[List[str]], cache: bool\n",
    "    ) -> DataFrame:\n",
    "        clean_text = \" \".join(text.split())\n",
    "        web_content = self._trim_text_from_end(\n",
    "            clean_text, self._max_tokens_of_web_content\n",
    "        )\n",
    "\n",
    "        sql_columns_hint = self._generate_sql_prompt(columns)\n",
    "\n",
    "        # Run the LLM chain to get an ingestion SQL query\n",
    "        tags = self._get_tags(cache)\n",
    "        temp_view_name = random_view_name()\n",
    "        llm_result = self._sql_llm_chain.run(\n",
    "            tags=tags,\n",
    "            query=desc,\n",
    "            web_content=web_content,\n",
    "            view_name=temp_view_name,\n",
    "            columns=sql_columns_hint,\n",
    "        )\n",
    "        sql_query = self._extract_code_blocks(llm_result)[0]\n",
    "        # The actual view name used in the SQL query may be different from the\n",
    "        # temp view name because of caching.\n",
    "        view_name = self._extract_view_name(sql_query)\n",
    "        formatted_sql_query = CodeLogger.colorize_code(sql_query, \"sql\")\n",
    "        self.log(f\"SQL query for the ingestion:\\n{formatted_sql_query}\")\n",
    "        self.log(f\"Storing data into temp view: {view_name}\\n\")\n",
    "        self._spark.sql(sql_query)\n",
    "        return self._spark.table(view_name)\n",
    "\n",
    "    def _get_df_schema(self, df: DataFrame) -> str:\n",
    "        return \"\\n\".join([f\"{name}: {dtype}\" for name, dtype in df.dtypes])\n",
    "\n",
    "    @staticmethod\n",
    "    def _trim_hash_id(analyzed_plan):\n",
    "        # Pattern to find strings like #59 or #2021\n",
    "        pattern = r\"#\\d+\"\n",
    "\n",
    "        # Remove matching patterns\n",
    "        trimmed_plan = re.sub(pattern, \"\", analyzed_plan)\n",
    "\n",
    "        return trimmed_plan\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_explain_string(df: DataFrame) -> str:\n",
    "        \"\"\"\n",
    "        Helper function to parse the content of the extended explain\n",
    "        string to extract the analyzed logical plan. As Spark does not provide\n",
    "        access to the logical plane without accessing the query execution object\n",
    "        directly, the value is extracted from the explain text representation.\n",
    "\n",
    "        :param df: The dataframe to extract the logical plan from.\n",
    "        :return: The analyzed logical plan.\n",
    "        \"\"\"\n",
    "        with contextlib.redirect_stdout(io.StringIO()) as f:\n",
    "            df.explain(extended=True)\n",
    "        explain = f.getvalue()\n",
    "        splits = explain.split(\"\\n\")\n",
    "        # The two index operations will fail if Spark changes the textual\n",
    "        # plan representation.\n",
    "        begin = splits.index(\"== Analyzed Logical Plan ==\")\n",
    "        end = splits.index(\"== Optimized Logical Plan ==\")\n",
    "        # The analyzed logical plan starts two lines after the section marker.\n",
    "        # The first line is the output schema.\n",
    "        return \"\\n\".join(splits[begin + 2 : end])\n",
    "\n",
    "    def _get_df_explain(self, df: DataFrame, cache: bool) -> str:\n",
    "        raw_analyzed_str = self._parse_explain_string(df)\n",
    "        tags = self._get_tags(cache)\n",
    "        return self._explain_chain.run(\n",
    "            tags=tags, input=self._trim_hash_id(raw_analyzed_str)\n",
    "        )\n",
    "\n",
    "    def _get_tags(self, cache: bool) -> Optional[List[str]]:\n",
    "        if self._enable_cache and not cache:\n",
    "            return SKIP_CACHE_TAGS\n",
    "        return None\n",
    "\n",
    "    def create_df(\n",
    "        self, desc: str, columns: Optional[List[str]] = None, cache: bool = True\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Create a Spark DataFrame by querying an LLM from web search result.\n",
    "\n",
    "        :param desc: the description of the result DataFrame, which will be used for\n",
    "                     web searching\n",
    "        :param columns: the expected column names in the result DataFrame\n",
    "        :param cache: If `True`, fetches cached data, if available. If `False`, retrieves fresh data and updates cache.\n",
    "\n",
    "        :return: a Spark DataFrame\n",
    "        \"\"\"\n",
    "        url = desc.strip()  # Remove leading and trailing whitespace\n",
    "        is_url = self._is_http_or_https_url(url)\n",
    "        # If the input is not a valid URL, use search tool to get the dataset.\n",
    "        if not is_url:\n",
    "            url = self._get_url_from_search_tool(desc, columns, cache)\n",
    "\n",
    "        self.log(f\"Parsing URL: {url}\\n\")\n",
    "        try:\n",
    "            response = requests.get(url, headers=self._HTTP_HEADER)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as http_err:\n",
    "            self.log(f\"HTTP error occurred: {http_err}\")\n",
    "            return\n",
    "        except Exception as err:\n",
    "            self.log(f\"Other error occurred: {err}\")\n",
    "            return\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # add url and page content to cache\n",
    "        if cache:\n",
    "            if self._cache.lookup(key=url):\n",
    "                page_content = self._cache.lookup(key=url)\n",
    "            else:\n",
    "                page_content = soup.get_text()\n",
    "                self._cache.update(key=url, val=page_content)\n",
    "        else:\n",
    "            page_content = soup.get_text()\n",
    "\n",
    "        # If the input is a URL link, use the title of web page as the\n",
    "        # dataset's description.\n",
    "        if is_url:\n",
    "            desc = soup.title.string\n",
    "        return self._create_dataframe_with_llm(page_content, desc, columns, cache)\n",
    "\n",
    "    def transform_df(self, df: DataFrame, desc: str, cache: bool = True) -> DataFrame:\n",
    "        \"\"\"\n",
    "        This method applies a transformation to a provided Spark DataFrame,\n",
    "        the specifics of which are determined by the 'desc' parameter.\n",
    "\n",
    "        :param df: The Spark DataFrame that is to be transformed.\n",
    "        :param desc: A natural language string that outlines the specific transformation to be applied on the DataFrame.\n",
    "        :param cache: If `True`, fetches cached data, if available. If `False`, retrieves fresh data and updates cache.\n",
    "\n",
    "        :return: Returns a new Spark DataFrame that is the result of applying the specified transformation\n",
    "                 on the input DataFrame.\n",
    "        \"\"\"\n",
    "        temp_view_name = random_view_name()\n",
    "        create_temp_view_code = CodeLogger.colorize_code(\n",
    "            f'df.createOrReplaceTempView(\"{temp_view_name}\")', \"python\"\n",
    "        )\n",
    "        self.log(f\"Creating temp view for the transform:\\n{create_temp_view_code}\")\n",
    "        df.createOrReplaceTempView(temp_view_name)\n",
    "        schema_str = self._get_df_schema(df)\n",
    "        tags = self._get_tags(cache)\n",
    "        llm_result = self._transform_chain.run(\n",
    "            tags=tags, view_name=temp_view_name, columns=schema_str, desc=desc\n",
    "        )\n",
    "        sql_query_from_response = self._extract_code_blocks(llm_result)[0]\n",
    "        # Replace the temp view name in case the view name is from the cache.\n",
    "        sql_query = replace_view_name(sql_query_from_response, temp_view_name)\n",
    "        formatted_sql_query = CodeLogger.colorize_code(sql_query, \"sql\")\n",
    "        self.log(f\"SQL query for the transform:\\n{formatted_sql_query}\")\n",
    "        return self._spark.sql(sql_query)\n",
    "\n",
    "    def explain_df(self, df: DataFrame, cache: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        This method generates a natural language explanation of the SQL plan of the input Spark DataFrame.\n",
    "\n",
    "        :param df: The Spark DataFrame to be explained.\n",
    "        :param cache: If `True`, fetches cached data, if available. If `False`, retrieves fresh data and updates cache.\n",
    "\n",
    "        :return: A string explanation of the DataFrame's SQL plan, detailing what the DataFrame is intended to retrieve.\n",
    "        \"\"\"\n",
    "        explain_result = self._get_df_explain(df, cache)\n",
    "        # If there is code block in the explain result, ignore it.\n",
    "        if \"```\" in explain_result:\n",
    "            summary = explain_result.split(\"```\")[-1]\n",
    "            return summary.strip()\n",
    "        else:\n",
    "            return explain_result\n",
    "\n",
    "    def plot_df(\n",
    "        self, df: DataFrame, desc: Optional[str] = None, cache: bool = True\n",
    "    ) -> None:\n",
    "        instruction = f\"The purpose of the plot: {desc}\" if desc is not None else \"\"\n",
    "        tags = self._get_tags(cache)\n",
    "        response = self._plot_chain.run(\n",
    "            tags=tags,\n",
    "            columns=self._get_df_schema(df),\n",
    "            explain=self._get_df_explain(df, cache),\n",
    "            instruction=instruction,\n",
    "        )\n",
    "        self.log(response)\n",
    "        codeblocks = self._extract_code_blocks(response)\n",
    "        code = \"\\n\".join(codeblocks)\n",
    "        try:\n",
    "            exec(compile(code, \"plot_df-CodeGen\", \"exec\"))\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Could not evaluate Python code\", e)\n",
    "        \n",
    "    def plot_df_geobins(\n",
    "        self, df: DataFrame, desc: Optional[str] = None, cache: bool = True\n",
    "    ) -> None:\n",
    "        instruction = f\"The purpose of the plot: {desc}\" if desc is not None else \"\"\n",
    "        tags = self._get_tags(cache)\n",
    "        response = self._geobins_plot_chain.run(\n",
    "            tags=tags,\n",
    "            columns=self._get_df_schema(df),\n",
    "            explain=self._get_df_explain(df, cache),\n",
    "            instruction=instruction,\n",
    "        )\n",
    "        self.log(response)\n",
    "        codeblocks = self._extract_code_blocks(response)\n",
    "        code = \"\\n\".join(codeblocks)\n",
    "        try:\n",
    "            exec(compile(code, \"plot_df_geobins-CodeGen\", \"exec\"))\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Could not evaluate Python code\", e)\n",
    "\n",
    "    def verify_df(self, df: DataFrame, desc: str, cache: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        This method creates and runs test cases for the provided PySpark dataframe transformation function.\n",
    "\n",
    "        :param df: The Spark DataFrame to be verified\n",
    "        :param desc: A description of the expectation to be verified\n",
    "        :param cache: If `True`, fetches cached data, if available. If `False`, retrieves fresh data and updates cache.\n",
    "        \"\"\"\n",
    "        tags = self._get_tags(cache)\n",
    "        llm_output = self._verify_chain.run(tags=tags, df=df, desc=desc)\n",
    "\n",
    "        codeblocks = self._extract_code_blocks(llm_output)\n",
    "        llm_output = \"\\n\".join(codeblocks)\n",
    "\n",
    "        self.log(f\"LLM Output:\\n{llm_output}\")\n",
    "\n",
    "        formatted_code = CodeLogger.colorize_code(llm_output, \"python\")\n",
    "        self.log(f\"Generated code:\\n{formatted_code}\")\n",
    "\n",
    "        locals_ = {}\n",
    "        try:\n",
    "            exec(compile(llm_output, \"verify_df-CodeGen\", \"exec\"), {\"df\": df}, locals_)\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Could not evaluate Python code\", e)\n",
    "        self.log(f\"\\nResult: {locals_['result']}\")\n",
    "\n",
    "    def udf(self, func: Callable) -> Callable:\n",
    "        from inspect import signature\n",
    "\n",
    "        desc = func.__doc__\n",
    "        func_signature = str(signature(func))\n",
    "        input_args_types = func_signature.split(\"->\")[0].strip()\n",
    "        return_type = func_signature.split(\"->\")[1].strip()\n",
    "        udf_name = func.__name__\n",
    "\n",
    "        code = self._udf_chain.run(\n",
    "            input_args_types=input_args_types,\n",
    "            desc=desc,\n",
    "            return_type=return_type,\n",
    "            udf_name=udf_name,\n",
    "        )\n",
    "\n",
    "        formatted_code = CodeLogger.colorize_code(code, \"python\")\n",
    "        self.log(f\"Creating following Python UDF:\\n{formatted_code}\")\n",
    "\n",
    "        locals_ = {}\n",
    "        try:\n",
    "            exec(compile(code, \"udf-CodeGen\", \"exec\"), globals(), locals_)\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Could not evaluate Python code\", e)\n",
    "        return locals_[udf_name]\n",
    "\n",
    "    def activate(self):\n",
    "        \"\"\"\n",
    "        Activates AI utility functions for Spark DataFrame.\n",
    "        \"\"\"\n",
    "        DataFrame.ai = AIUtils(self)\n",
    "        # Patch the Spark Connect DataFrame as well.\n",
    "        try:\n",
    "            from pyspark.sql.connect.dataframe import DataFrame as CDataFrame\n",
    "\n",
    "            CDataFrame.ai = AIUtils(self)\n",
    "        except ImportError:\n",
    "            self.log(\n",
    "                \"The pyspark.sql.connect.dataframe module could not be imported. \"\n",
    "                \"This might be due to your PySpark version being below 3.4.\"\n",
    "            )\n",
    "\n",
    "    def commit(self):\n",
    "        \"\"\"\n",
    "        Commit the staging in-memory cache into persistent cache, if cache is enabled.\n",
    "        \"\"\"\n",
    "        if self._cache is not None:\n",
    "            self._cache.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f25de7",
   "metadata": {},
   "source": [
    "## Running Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7507691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"GeoLLM\").getOrCreate()\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_base=\"https://azure-openai-personal-stylist.openai.azure.com/\",\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    openai_api_key=\"d6b01920a9f64098994de5dc830b8a94\",\n",
    "    openai_api_type=\"azure\",\n",
    ")\n",
    "\n",
    "geollm = GeoLLM(llm, verbose=True)\n",
    "geollm.activate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5633ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \",\".join(\n",
    "    [\n",
    "        \"`ADDRESS` string\",\n",
    "        \"`SUBURB` string\",\n",
    "        \"`PRICE` double\",\n",
    "        \"`BEDROOMS` integer\",\n",
    "        \"`BATHROOMS` integer\",\n",
    "        \"`GARAGE` integer\",\n",
    "        \"`LAND_AREA` double\",\n",
    "        \"`FLOOR_AREA` double\",\n",
    "        \"`BUILD_YEAR` int\",\n",
    "        \"`CBD_DIST` double\",\n",
    "        \"`NEAREST_STN` string\",\n",
    "        \"`NEAREST_STN_DIST` string\",\n",
    "        \"`DATE_SOLD` string\",\n",
    "        \"`POSTCODE` string\",\n",
    "        \"`LATITUDE` double\",\n",
    "        \"`LONGITUDE` double\",\n",
    "        \"`NEAREST_SCH` string\",\n",
    "        \"`NEAREST_SCH_DIST` double\",\n",
    "        \"`NEAREST_SCH_RANK` int\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "df = spark.read.csv(r\"..\\data\\all_perth_310121_new.csv\", header=True, schema=schema).cache()\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8709d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "geollm.plot_df(\n",
    "    df, \"show distribution of NEAREST_SCH_DIST less than 8 miles in 32 bins\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cced0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "geollm.plot_df_geobins(\n",
    "    df, \"apply a spatial binning on the dataframe using lon column and lat column and cell size is 300 and max cell count is 70\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
